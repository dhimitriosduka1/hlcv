{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "\n",
    "# This allows you to edit the imported modules (e.g code in other files) without having to restart the kernel.\n",
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have shown staggering performances in various learning tasks, including computer vision, natural language processing, and sound processing. They have made the model design more flexible by enabling end-to-end training.\n",
    "\n",
    "In this exercise, we get to have a first hands-on experience with neural network training. Many frameworks (e.g., PyTorch, Tensorflow, Caffe) allow easy usage of deep neural networks without precise knowledge of the inner workings of backpropagation and gradient descent algorithms. While these are very useful tools, it is important to get a good understanding of how to implement basic network training from scratch before using these libraries to speed up the process. For this purpose, we will implement a simple two-layer neural network and its training algorithm based on back-propagation using only basic matrix operations in questions 1 to 3. In question 4, we will use a popular deep learning library, PyTorch, to do the same and understand the advantages offered by using such tools.\n",
    "\n",
    "As a benchmark to test our models, we consider an image classification task using the widely used CIFAR-10 dataset. This dataset consists of 50000 training images of 32x32 resolution with 10 object classes, namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. The task is to code and train a parametrized model for classifying those images. This involves\n",
    "\n",
    "- Implementing the feedforward model (Question 1).\n",
    "- Implementing the backpropagation algorithm (gradient computation) (Question 2).\n",
    "- Training the model using stochastic gradient descent and improving the model training with better hyper-parameters (Question 3).\n",
    "- Using the PyTorch Library to implement the above and experiment with deeper networks (Question 4).\n",
    "\n",
    "A note on notation: Throughout the exercise, notation $v_i$ is used to denote the $i$-th element of vector $v$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Implementing the feedforward model (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question, we will implement a two-layered neural network architecture and the loss function to train it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load image from ./data/exercise-2/fig1.png You can view it manually](./data/exercise-2/fig1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model architecture.** Our architecture is shown in Fig.1. It has an input layer and two model layers â€“ a hidden and an output layer. We start with randomly generated toy inputs of four dimensions and the number of classes K = 3 to build our model in Q1 and Q2, and in Q3 use images from the CIFAR-10 dataset to test our model on a real-world task. Hence input layer is 4-dimensional for now.\n",
    "\n",
    "In the hidden layer, there are 10 units. The input layer and the hidden\n",
    "layer are connected via linear weighting matrix $W^{(1)}\\in\\mathbb{R}^{10\\times\n",
    "4}$ and the bias term $b^{(1)}\\in\\mathbb{R}^{10}$. The parameters $W^{(1)}$\n",
    "and $b^{(1)}$ are to be learnt later on. A linear operation is performed,\n",
    "$W^{(1)}x+b^{(1)}$, resulting in a 10 dimensional vector $z^{(2)}$. It is then\n",
    "followed by a relu non-linear activation $\\phi$, applied element-wise on each\n",
    "unit, resulting in the activations $a^{(2)} = \\phi(z^{(2)})$. Relu function has\n",
    "the following form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\phi(u) =  \\begin{cases}\n",
    "      u, & \\text{if}\\ u\\geq0 \\\\\n",
    "      0, & \\text{if}\\ u <0\n",
    "    \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "A similar linear operation is performed on $a^{(2)}$, resulting in $z^{(3)}=W^{(2)}a^{(2)}+b^{(2)}$, where $W^{(2)}\\in\\mathbb{R}^{3\\times 10}$ and $b^{(2)}\\in\\mathbb{R}^{3}$; it is followed by the softmax activation to result in $a^{(3)}=\\psi(z^{(3)})$. The softmax function is defined by:\n",
    "\\begin{equation}\n",
    "\\psi(u)_i =  \\frac{\\exp^{u_i}}{\\sum_j{\\exp^{u_j}}} \n",
    "\\end{equation}\n",
    "\n",
    "The final functional form of our model is thus defined by\n",
    "\n",
    "\\begin{align*}\n",
    "a^{(1)} &= x \\\\ \\\n",
    "z^{(2)} &= W^{(1)}a^{(1)}+b^{(1)} \\\\\n",
    "a^{(2)} &= \\phi(z^{(2)}) \\\\\n",
    "z^{(3)} &= W^{(2)}a^{(2)}+b^{(2)} \\\\\n",
    "f_\\theta(x) := a^{(3)} &= \\psi(z^{(3)}),\n",
    "\\end{align*}\n",
    "\n",
    "which takes a flattened 4 dimensional vector as input and outputs a $3$ dimensional vector, each entry in the output $f_k(x)$ representing the probability of image $x$ corresponding to the class $k$. We summarily indicate all the network parameters by $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation.** We are now ready to implement the feedforward neural network.\n",
    "\n",
    "a) Implement the feedforward model. Verify that the scores you generate for the toy inputs match the correct scores. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import models.twolayernet.model as module_twolayernet\n",
    "from utils.utils import seed_everything, init_toy_data, rel_error\n",
    "from utils.gradient_check import eval_numerical_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(1)\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "# You would need to implement the forward() pass of the TwoLayerNetv1 module.\n",
    "net = module_twolayernet.TwoLayerNetv1(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "X, y = init_toy_data(num_inputs, input_size)\n",
    "\n",
    "scores = net.forward(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('Correct scores:')\n",
    "correct_scores = np.asarray([\n",
    " [0.36446210, 0.22911264, 0.40642526],\n",
    " [0.47590629, 0.17217039, 0.35192332],\n",
    " [0.43035767, 0.26164229, 0.30800004],\n",
    " [0.41583127, 0.29832280, 0.28584593],\n",
    " [0.36328815, 0.32279939, 0.31391246]])\n",
    "print(correct_scores)\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))\n",
    "\n",
    "assert math.isclose(np.sum(np.abs(scores - correct_scores)), 0,  abs_tol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We later guide the neural network parameters\n",
    "$\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$ to fit the given data and label\n",
    "pairs. We do so by minimising the loss function. A popular choice of the loss\n",
    "function for training neural networks for multi-class classification is the\n",
    "cross-entropy loss. For a single input sample $x_i$, with label $y_i$, the loss\n",
    "function is defined as:\n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta, x_i, y_i) &= -\\log{P(Y=y_i,X=x_i)} \\\\\n",
    "                    &= -\\log{f_\\theta(x_i)_{y_i}} \\\\\n",
    "                    &= -\\log{\\psi(z^{(3)})_{y_i}} \\\\\n",
    "J(\\theta, x_i, y_i) &= -\\log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum^K_j{\\exp^{z^{(3)}_j}}}\\right]\n",
    "\\end{align}\n",
    "\n",
    "Averaging over the whole training set, we get \n",
    "\n",
    "\\begin{align}\n",
    "J(\\theta,\\{x_i,y_i\\}_{i=1}^{N}) = \\frac{1}{N} \\sum_{i=1}^N \n",
    "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum_j^K{\\exp^{z^{(3)}_j}}}\\right],\n",
    "\\end{align}\n",
    "\n",
    "where $K$ is the number of classes. Note that if the model has perfectly fitted\n",
    "to the data (i.e. $f_\\theta^k(x_i)=1$ whenever $x_i$ belongs to class $k$ and 0\n",
    "otherwise), then $J$ attains the minimum of $0$. \n",
    "\n",
    "\n",
    "Apart from trying to correctly predict the label, we have to prevent\n",
    "overfitting the model to the current training data.  This is done by encoding\n",
    "our prior belief that the correct model should be simple (Occam's razor); we\n",
    "add an $L_2$ regularisation term over the model parameters $\\theta$.\n",
    "Specifically, the loss function is defined by:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{J}(\\theta) = \\frac{1}{N} \\sum_{i=1}^N\n",
    "-log\\left[ \\frac{\\exp^{z^{(3)}_{y_i}}}{\\sum_j{\\exp^{z^{(3)}_j}}}\\right]\n",
    "+\\lambda \\left(||W^{(1)}||_2^2 + ||W^{(2)}||_2^2 \\right),\n",
    "\\end{align}\n",
    "\n",
    "where $||\\cdot||_2^2$ is the squared $L_2$ norm. For example,\n",
    "\n",
    "\\begin{align}\n",
    "||W^{(1)}||_2^2 = \n",
    "\\sum_{p=1}^{10} \\sum_{q=1}^{4} W_{pq}^{(1)2}\n",
    "\\end{align}\n",
    "\n",
    "By changing the value of $\\lambda$ it is possible to give weights to your prior belief on the degree of simplicity (regularity) of the true model. \n",
    "\n",
    "Implement the final loss function and let it return the loss value. Verify the code by\n",
    "running and matching the output cost $1.30378789133$. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_v2 = module_twolayernet.TwoLayerNetv2(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1) \n",
    "loss = net_v2.compute_loss(X, y, reg=0.05)\n",
    "correct_loss = 1.30378789133 # check this number with your implementation\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))\n",
    "\n",
    "assert math.isclose(np.sum(np.abs(loss - correct_loss)), 0,  abs_tol=1e-6), 'The error with respect to the correct value is too high'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Backpropagation (15 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model by solving\n",
    "\\begin{align}\n",
    "\\underset{\\theta}{\\min} \\,\\,  \\tilde{J}(\\theta)\n",
    "\\end{align}\n",
    "via stochastic gradient descent. Therefore, We need an efficient computation of the gradients $\\nabla_\\theta \\tilde{J}(\\theta)$. We use backpropagation of top layer error signals to the parameters $\\theta$ at different layers.\n",
    "\n",
    "In this question, you will be required to implement the backpropagation algorithm yourself from pseudocode. We will give a high-level description of what is happening in each line.\n",
    "\n",
    "For those who are interested in the robust derivation of the algorithm, we include the optional exercise on the derivation of the backpropagation algorithm. A piece of prior knowledge of standard vector calculus, including the chain rule, would be helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backpropagation.** The backpropagation algorithm is simply a sequential application of the chain rule. It is applicable to any (sub-) differentiable model that is a composition of simple building blocks. In this exercise, we focus on the architecture with stacked layers of linear transformation + relu non-linear activation.\n",
    "\n",
    "The intuition behind the backpropagation algorithm is as follows. Given a training example $(x, y)$, we first run the feedforward to compute all the activations throughout the network, including the output value of the model $f_\\theta(x)$ and the loss $J$. Then, for each parameter in the model, we want to compute the effect that parameter has on the loss. This is done by computing the derivatives of the loss w.r.t for each model parameter.\n",
    "\n",
    "The backpropagation algorithm is performed from the top of the network (loss layer) to the bottom. It sequentially computes the gradient of the loss function with respect to each layer's activations and parameters.\n",
    "\n",
    "Letâ€™s start by deriving the gradients of the un-regularized loss function w.r.t final layer activations $z^{(3)}$. We will then use this in the chain rule to compute analytical expressions for gradients of all the model parameters.\n",
    "\n",
    "(a) Verify that the loss function (in Q1) has the gradient w.r.t $z^{(3)}$ as below.\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N}\\left(\\psi(z^{(3)}) - \\Delta\\right), \n",
    "\\end{equation}\n",
    "where $\\Delta$ is a matrix of $N\\times K$ dimensions with \n",
    "\\begin{align}\n",
    "        \\Delta_{ij} = 1, & \\text{if}\\ y_i =j \\\\\n",
    "                0, & \\text{otherwise}\n",
    "\\end{align}\n",
    "and $z^{(3)}$ here refers to all the activations of the training set.\n",
    "\\begin{align}\n",
    "        z^{(3)} \\in \\mathbb R^{N \\times K}\n",
    "\\end{align}\n",
    "(please write your answer in the block below, or attach an image in the same cell, 2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the case when $N=1$ i.e. there is only one input data.\n",
    "The loss can be written as:\n",
    "\\begin{align*}\n",
    "    J\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\sum_{i = 1}^N\\, -y_i\\log a^{(3)}_i\\\\\n",
    "    &= -\\log\\,a^{(3)}_k\n",
    "\\end{align*}\n",
    "where $a_k$ is the output probability of the true class $k$ (i.e, $y_k = 1$). For the remaining terms, $j \\neq k$, $y_j = 0$. Thus, the gradient vector would be:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial a^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\begin{pmatrix}\n",
    "            \\vdots\\\\    \n",
    "            0\\\\\n",
    "            -\\frac{1}{a^{(3)}_k}\\\\\n",
    "            0\\\\\n",
    "            \\vdots\\\\\n",
    "        \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "Given $a^{(3)}$ and $z^{(3)} \\in \\mathbb{R}^K$, the Softmax function is applied to $ z^{(3)}$ to obtain $ a^{(3)}$. The Softmax function is defined as:\n",
    "\n",
    "\\begin{align*}\n",
    "    a_i^{(3)} = \\psi(z_i^{(3)}) = \\frac{e^{z_i^{(3)}}}{\\sum_{j=1}^{K} e^{z_j^{(3)}}} \\quad \\text{for } i = 1, 2, \\ldots, K\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We need to compute the Jacobian matrix $\\frac{\\partial a^{(3)}}{\\partial z^{(3)}}$, which is a $K \\times K$ matrix where each element represents the partial derivative of one component of $a^{(3)}$ with respect to one component of $z^{(3)}$.\n",
    "\n",
    "Let's denote the elements of this Jacobian matrix as $J_{ij} = \\frac{\\partial a_i^{(3)}}{\\partial z_j^{(3)}}$. The elements can be categorized into two cases: $i = j$ and $i \\neq j$.\n",
    "\n",
    "For the diagonal elements, we have $i = j$:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a^{(3)}_i}{\\partial z^{(3)}_i} \n",
    "    &= \\frac{e^{z^{(3)}_i}}{\\sum_j^K\\,e^{z^{(3)}_j}} + (-1)(e^{z^{(3)}_i})(\\sum_j^K\\,e^{z^{(3)}_j})^{-2}(e^{z^{(3)}_i})\\\\\n",
    "    &= \\frac{e^{z^{(3)}_i}}{\\sum_j^K\\,e^{z^{(3)}_j}} - \\frac{(e^{z^{(3)}_i})^2}{(\\sum_j^K\\,e^{z^{(3)}_j})^2}\\\\\n",
    "    &= \\frac{e^{z^{(3)}_i}}{\\sum_j^K\\,e^{z^{(3)}_j}} - (\\frac{e^{z^{(3)}_i}}{\\sum_j^K\\,e^{z^{(3)}_j}})^2\\\\\n",
    "    &= \\psi(z^{(3)}_i) - \\psi(z^{(3)}_i)^2\\\\\n",
    "    &= \\psi(z^{(3)}_i)(1 - \\psi(z^{(3)}_i))\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "For the off-diagonal elements, we have $i \\neq j$:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a^{(3)}_i}{\\partial z^{(3)}_j}\n",
    "    &= (-1)(e^{z^{(3)}_i})(\\sum_l^K\\,e^{z^{(3)}_l})^{-2}(e^{z^{(3)}_j})\\\\\n",
    "    &= -\\frac{e^{z^{(3)}_i}}{\\sum_l^K\\,e^{z^{(3)}_l}}\\frac{e^{z^{(3)}_j}}{\\sum_l^K\\,e^{z^{(3)}_l}}\\\\\n",
    "    &= -\\psi(z^{(3)}_i)\\psi(z^{(3)}_j)\\\\\n",
    "\\end{align*}\n",
    "\n",
    "Putting it all together the Jacobian matrix (e.g. $K = 3$) can be represented as:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial a^{(3)}}{\\partial z^{(3)}} =\n",
    "    \\begin{pmatrix}\n",
    "    \\psi(z^{(3)}_1) - \\psi(z^{(3)}_1)^2 & -\\psi(z^{(3)}_1)\\psi(z^{(3)}_2) & -\\psi(z^{(3)}_1)\\psi(z^{(3)}_3)\\\\\n",
    "    -\\psi(z^{(3)}_2)\\psi(z^{(3)}_1) & \\psi(z^{(3)}_2) - \\psi(z^{(3)}_2)^2 & -\\psi(z^{(3)}_2)\\psi(z^{(3)}_3) \\\\\n",
    "    -\\psi(z^{(3)}_3)\\psi(z^{(3)}_1) & -\\psi(z^{(3)}_3)\\psi(z^{(3)}_2) & \\psi(z^{(3)}_3) - \\psi(z^{(3)}_3)^2 \\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "By chain rule, we know that (e.g. $K = 3$, true label is $2$ i.e. $y_2 = 1$)\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\frac{\\partial a^{(3)}}{\\partial z^{(3)}} \\frac{\\partial J}{\\partial a^{(3)}} \\left(\\{x_i,y_i\\}_{i=1}^{N}\\right)\\\\\n",
    "    &= \\begin{pmatrix}\n",
    "    \\psi(z^{(3)}_1) - \\psi(z^{(3)}_1)^2 & -\\psi(z^{(3)}_1)\\psi(z^{(3)}_2) & -\\psi(z^{(3)}_1)\\psi(z^{(3)}_3)\\\\\n",
    "    -\\psi(z^{(3)}_2)\\psi(z^{(3)}_1) & \\psi(z^{(3)}_2) - \\psi(z^{(3)}_2)^2 & -\\psi(z^{(3)}_2)\\psi(z^{(3)}_3) \\\\\n",
    "    -\\psi(z^{(3)}_3)\\psi(z^{(3)}_1) & -\\psi(z^{(3)}_3)\\psi(z^{(3)}_2) & \\psi(z^{(3)}_3) - \\psi(z^{(3)}_3)^2 \\\\\n",
    "    \\end{pmatrix}\n",
    "    \\begin{pmatrix}\n",
    "    0\\\\\n",
    "    -\\frac{1}{\\psi(z^{(3)}_2)}\\\\\n",
    "    0\\\\\n",
    "    \\end{pmatrix}\\\\\n",
    "    &= \\begin{pmatrix}\n",
    "    \\psi(z^{(3)}_1)\\\\\n",
    "    \\psi(z^{(3)}_2) - 1\\\\\n",
    "    \\psi(z^{(3)}_3)\\\\\n",
    "    \\end{pmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "When $N > 1$, for each input $\\left(x_i, y_i \\right)$, the corresponding gradient contribution is:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\psi(z^{(3)}) - \\Delta_i\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Delta_i$ is the one-hot encoded vector of the true class labels for the $i$-th example.\n",
    "\n",
    "Averaging over all $N$ examples, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N} \\sum_{i = 1}^{N}(\\psi(z^{(3)}) - \\Delta_i) = \\frac{1}{N}(\\psi(z^{(3)}) - \\Delta)\n",
    "\\end{align*}\n",
    "\n",
    "Thus, the final gradient of the loss function with respect to $z^{(3)}$ is:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N}(\\psi(z^{(3)}) - \\Delta)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\Delta$ is a matrix of $N \\times K$ dimensions with: \n",
    "\n",
    "\\begin{align*}\n",
    "    \\Delta_{ij} = \n",
    "    \\begin{cases}\n",
    "        1, & \\text{if } y_i = j \\\\\n",
    "        0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) To compute the effect of the weight matrix $W^{(2)}$ on the loss (in Q1) incurred by the network, we compute the\n",
    "partial derivatives of the loss function with respect to $W^{2}$.  This is done\n",
    "by applying the chain rule. Verify that the partial derivative of the loss w.r.t $W^{(2)}$ is  \n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial W^{(2)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} \\\\\n",
    "&= \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)'}\n",
    "\\end{align}\n",
    "\n",
    "Similarly, verify that the regularized loss has the derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac{1}{N} (\\psi(z^{(3)}) - \\Delta) a^{(2)'} + 2\\lambda W^{(2)}\n",
    "\\end{align}\n",
    "\n",
    "(please write your answer in the block below, or attach an image in the same cell, 2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the $\\frac{\\partial J}{\\partial W^{(2)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right)$, we need to make use of the chain rule.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial W^{(2)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) &= \\frac{\\partial J}{\\partial z^{(3)}} \\cdot \\frac{\\partial z^{(3)}}{\\partial W^{(2)}}\n",
    "\\end{align*}\n",
    "\n",
    "We already know the derivative of $\\frac{\\partial J}{\\partial z^{(3)}}$ from the previous exercise. So, we just need to calculate the remaining term. We know that $z^{(3)} = W^{(2)} a^{(2)} + b^{(2)}$. For simplicity, we consider the gradient w.r.t an element $W^{(2)}_{ij}$:  \n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z_i^{(3)}}{\\partial W^{(2)}_{kj}} = \n",
    "    \\begin{cases}\n",
    "        a_j^{(2)}, & \\text{if } k = i \\\\\n",
    "        0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "Hence,\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z^{(3)}}{\\partial W^{(2)}} = a^{(2)}\n",
    "\\end{align*}\n",
    "\n",
    "Before writing down the final formula, we need to be careful about the dimensions of the matrices. As stated in the previous exercise, $\\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right)$ is a matrix of $N \\times K$ dimensions. $a^{(2)}$ on the other hand is a matrix of $N \\times H$ dimensions. By taking the transpose of $a^{(2)}$, we align the dimensions correctly. Putting it all together, we get:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial z^{(3)}}\\left(\\{x_i,y_i\\}_{i=1}^{N}\\right) = \\frac{1}{N} \\left(a^{(2)} \\right)^T\\left(\\psi(z^{(3)}) - \\Delta\\right)\n",
    "\\end{align*}\n",
    "\n",
    "For the regularized loss function, we have:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\tilde{J} = J + \\lambda \\| W^{(2)} \\|_2^2\n",
    "\\end{align*}\n",
    "where \\(J\\) is the original loss and $\\left(\\lambda \\| W^{(2)} \\|_2^2\\right)$ is the L2 regularization term.\n",
    "\n",
    "The gradient of the regularized loss $\\tilde{J}$ with respect to $W^{(2)}$ is given by:\n",
    "\\begin{align*}    \n",
    "    \\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac{\\partial J}{\\partial W^{(2)}} + \\frac{\\partial}{\\partial W^{(2)}} \\left( \\lambda \\| W^{(2)} \\|_2^2 \\right)\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "We already know that:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial W^{(2)}} = \\frac{1}{N} \\left(a^{(2)} \\right)^T (\\psi(z^{(3)}) - \\Delta)\n",
    "\\end{align*}\n",
    "\n",
    "For the regularization term, the gradient is:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial}{\\partial W^{(2)}} \\left( \\lambda \\| W^{(2)} \\|_2^2 \\right) = 2\\lambda W^{(2)}\n",
    "\\end{align*}\n",
    "\n",
    "Combining these, we get the gradient of the regularized loss:\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\tilde{J}}{\\partial W^{(2)}} = \\frac{1}{N} \\left(a^{(2)} \\right)^T (\\psi(z^{(3)}) - \\Delta) + 2\\lambda W^{(2)}\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) We can repeatedly apply chain rule as discussed above to obtain the derivatives of the loss with respect to all the parameters of the model $\\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)})$.\n",
    "Derive the expressions for the derivatives of the regularized loss (in Q1) w.r.t $W^{(1)}$, $b^{(1)}$, $b^{(2)}$ now.\n",
    "(please write your answer in the block below, or attach an image in the same cell, 6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer 2c\n",
    "<!-- $\n",
    "    \\renewcommand{\\bb}{b^{(2)}}\n",
    "    \\renewcommand{\\zc}{z^{(3)}}\n",
    "    \\renewcommand{\\Wb}{W^{(2)}}\n",
    "    \\newcommand{\\ab}{a^{(2)}}\n",
    "$ -->\n",
    "\n",
    "When $N = 1$, from the forumla of $z_i^{(3)}$ above we can derive that:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z_i^{(3)}}{\\partial b^{(2)}_k} = \\begin{cases}\n",
    "        1 & \\text{if } i = k\\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} = I\n",
    "\\end{align*}\n",
    "\n",
    "Thus the $N > 1$ case Jacobian matrix $\\frac{\\partial J}{\\partial b^{(2)}}$ can be expressed as:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial J}{\\partial b^{(2)}}\\right)_i &= \\sum_{n}^{N}\\left(\\frac{\\partial J}{\\partial z^{(3)}} \\cdot I\\right)_{ni}\\\\\n",
    "    &= \\sum_{n}^{N}\\left(\\frac{\\partial J}{\\partial z^{(3)}}\\right)_{ni}\\\\\n",
    "    &= \\frac{1}{N}\\sum_{n}^{N}\\left(\\psi(z^{(3)}) - \\Delta\\right)_{ni}\n",
    "\\end{align*}\n",
    "\n",
    "Similarly:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial z_i^{(3)}}{\\partial a^{(2)}_j} = W^{(2)}_{ij}\n",
    "\\end{align*}\n",
    "\n",
    "so for $N > 1$ we have\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial J}{\\partial a^{(2)}_j} = \\frac{\\partial J}{\\partial z_i^{(3)}}  W^{(2)} = \n",
    "    \\frac{1}{N}\\left(\\psi(z^{(3)}) - \\Delta\\right) W^{(2)}\n",
    "\\end{align*}\n",
    "\n",
    "The ReLU function $\\phi$ is element-wise i.e. $\\phi : \\mathbb{R}^H \\mapsto \\mathbb{R}^H$\n",
    "thus for $N = 1$, $\\frac{\\partial a^{(2)}}{\\partial z^{(2)}} \\in \\mathbb{R}^{H\\times H}$ is a diagonal matrix with its diagonal elements\n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial a^{(2)}}{\\partial z^{(2)}}\\right)_{ii} = \\begin{cases}\n",
    "        1 & \\text{if } z^{(2)}_{i} > 0\\\\\n",
    "        0 & \\text{otherwise}\n",
    "    \\end{cases} = \\mathrm{diag}(\\mathrm{sign(z^{(2)})})\n",
    "\\end{align*}\n",
    "\n",
    "It is a bit tricky for the $N > 1$ case as the upstream gradient $\\frac{\\partial J}{\\partial a^{(2)}}$ is a matrix of size $N\\times H$\n",
    "where each row $(\\frac{\\partial J}{\\partial a^{(2)}})_n\\in\\mathbb{R}^{1\\times H}$.\n",
    "\n",
    "For each input/row we have \n",
    "\n",
    "\\begin{align*}\n",
    "    (\\frac{\\partial J}{\\partial z^{(2)}})_n = (\\frac{\\partial J}{\\partial a^{(2)}})_n \\cdot \\mathrm{diag}(\\mathrm{sign(z^{(2)}_n)})\n",
    "\\end{align*}\n",
    "\n",
    "with this upstream gradient $\\frac{\\partial J}{\\partial z^{(2)}}$ the gradient for $W^{(1)}$ and $b^{(1)}$ can be calculated in a similar way as $W^{(2)}$ and $b^{(2)}$, literally\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\tilde{J}}{\\partial W^{(1)}} = X^\\top\\left(\\frac{\\partial J}{\\partial z^{(2)}}\\right) + 2\\lambda W^{(1)}\n",
    "\\end{align*}\n",
    "\n",
    "and \n",
    "\n",
    "\\begin{align*}\n",
    "    \\left(\\frac{\\partial J}{\\partial b^{(1)}}\\right)_i = \\sum_{n}^{N}\\left(\\frac{\\partial J}{\\partial z^{(2)}}\\right)_{ni}\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Using the expressions you obtained for the derivatives of the loss w.r.t model parameters, implement the back-propagation algorithm. Run the code and verify that the gradients you obtained are correct using numerical gradients (already\n",
    "implemented in the code). The maximum relative error between the gradients you compute and the numerical gradients should be less than 1e-8 for all parameters.\n",
    "(5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_v3 = module_twolayernet.TwoLayerNetv3(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "loss, grads = net_v3.back_propagation(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net_v3.back_propagation(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net_v3.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Stochastic gradient descent training (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have implemented the backpropagation algorithm for computing the parameter gradients and have verified that it indeed gives the correct gradient. We are now ready to train the network. We solve Eq.15 with the stochastic gradient descent.\n",
    "\n",
    "Typically neural networks are large and are trained with millions of data\n",
    "points. It is thus often infeasible to compute the gradient $\\nabla_\\theta\n",
    "\\tilde{J}(\\theta)$ that requires the accumulation of the gradient over the\n",
    "entire training set. Stochastic gradient descent addresses this problem by\n",
    "simply accumulating the gradient over a small random subset of the training\n",
    "samples (minibatch) at each iteration. Specifically, the algorithm is as\n",
    "follows,"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load the image from ./data/exercise-2/alg1.png You can view it manually](./data/exercise-2/alg1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the gradient $\\nabla_\\theta \\tilde{J}(\\theta,\\{(X^\\prime_j,y^\\prime_j)\\}_{j=1}^B)$ is computed only on the current randomly sampled batch.\n",
    "\n",
    "Intuitively, $v = -\\nabla_\\theta \\tilde{J}(\\theta^{(t-1)})$ gives the direction\n",
    "to which the loss $\\tilde{J}$ decreases the most (locally), and therefore we\n",
    "follow that direction by updating the parameters towards that direction\n",
    "$\\theta^{(t)} = \\theta^{(t-1)} + v$. \n",
    "\n",
    "a) Implement the stochastic gradient descent algorithm and run the training on the toy data. Your model\n",
    "should be able to  obtain loss <= 0.02 on the training set and the training\n",
    "curve should look similar to the one shown in figure 2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Failed to load the image from ./data/exercise-2/fig2.png Please view it yourself](./data/exercise-2/fig2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_v4 = module_twolayernet.TwoLayerNetv4(input_size=input_size, hidden_size=hidden_size, output_size=num_classes, std=1e-1)\n",
    "stats = net_v4.train(X, y, X, y,\n",
    "                    learning_rate=1e-1, reg=5e-6,\n",
    "                    num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) We are now ready to train our model on a real image dataset. For this, we will use\n",
    "the CIFAR-10 dataset.  Since the images are of size $32\\times 32$ pixels with 3\n",
    "color channels, this gives us 3072 input layer units, represented by a vector\n",
    "$x\\in\\mathbb{R}^{3072}$. The code to load the data and train the model is provided with\n",
    "some default hyperparameters. With default\n",
    "hyperparameters, if previous questions have been done correctly, you should get\n",
    "a validation set accuracy of about 29\\%. This is very poor.\n",
    "Your task is to debug the model training and come up with better hyperparameters\n",
    "to improve the performance on the validation set.\n",
    "Visualize the training and validation performance curves to help with this analysis.\n",
    "There are several pointers provided in the comments to \n",
    "help you understand why the network might be underperforming.\n",
    "Once you have tuned your hyperparameters, and get validation accuracy greater\n",
    "than 48\\% run your best model on the test set once and report the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download CIFAR-10 using this link: <http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz>**\n",
    "<br/>\n",
    "**Decompress the downloaded dataset, and put the `cifar-10-batches-py` folder in the folder `data/exercise-2`**\n",
    "\n",
    "You can also try the next two commented lines (simply uncomment them and run the cell). The commands are tested on Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
    "# !tar -xzf cifar-10-python.tar.gz -C ./data/exercise-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import show_net_weights\n",
    "from utils.data_utils import get_CIFAR10_data\n",
    "from utils.vis_utils import visualize_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the data\n",
    "# Now that you have implemented a two-layer network that passes\n",
    "# gradient checks and works on toy data, it's time to load up our favorite\n",
    "# CIFAR-10 data so we can use it to train a classifier on a real dataset.\n",
    "# Invoke the get_CIFAR10_data function to get our data.\n",
    "\n",
    "# Load the raw CIFAR-10 data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(\n",
    "    data_dir=os.path.join(\".\", \"data\", \"exercise-2\", \"cifar-10-batches-py\")\n",
    ")\n",
    "\n",
    "print(\"Train data shape: \", X_train.shape)\n",
    "print(\"Train labels shape: \", y_train.shape)\n",
    "print(\"Validation data shape: \", X_val.shape)\n",
    "print(\"Validation labels shape: \", y_val.shape)\n",
    "print(\"Test data shape: \", X_test.shape)\n",
    "print(\"Test labels shape: \", y_test.shape)\n",
    "\n",
    "# Visualize some images to get a feel for the data\n",
    "plt.imshow(visualize_grid(X_train[:100, :].reshape(100, 32, 32, 3), padding=3).astype(\"uint8\"))\n",
    "plt.gca().axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "######################## Train a network ########################\n",
    "# To train our network we will use SGD. In addition, we will\n",
    "# adjust the learning rate with an exponential learning rate schedule as\n",
    "# optimization proceeds; after each epoch, we will reduce the learning rate by\n",
    "# multiplying it by a decay rate.\n",
    "\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 50\n",
    "num_classes = 10\n",
    "net = module_twolayernet.TwoLayerNetv4(\n",
    "    input_size=input_size, hidden_size=hidden_size, output_size=num_classes\n",
    ")\n",
    "# Train the network\n",
    "stats = net.train(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    num_iters=1000,\n",
    "    batch_size=200,\n",
    "    learning_rate=1e-4,\n",
    "    learning_rate_decay=0.95,\n",
    "    reg=0.25,\n",
    "    verbose=True,\n",
    ")\n",
    "# a larger batch size and a larger learning rate\n",
    "# e.g. batch_size=400, learning_rate=1e-3\n",
    "# this gives you validation accuracy > 0.48\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_acc = (net.predict(X_val) == y_val).mean()\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "\n",
    "######################## Debug the training ##################################\n",
    "# With the default parameters we provided above, you should get a validation\n",
    "# accuracy of about 0.29 on the validation set. This isn't very good.\n",
    "#\n",
    "# One strategy for getting insight into what's wrong is to plot the loss\n",
    "# function and the accuracies on the training and validation sets during\n",
    "# optimization.\n",
    "# Plot the loss function and train / validation accuracies\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(2 * 5, 5 * 1), dpi=100)\n",
    "axes[0].plot(stats[\"loss_history\"])\n",
    "axes[0].set_title(\"Loss history\")\n",
    "axes[0].set_xlabel(\"Iteration\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axes[1].plot(stats[\"train_acc_history\"], label=\"train\")\n",
    "axes[1].plot(stats[\"val_acc_history\"], label=\"val\")\n",
    "axes[1].set_title(\"Classification accuracy history\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Classification accuracy\")\n",
    "axes[1].legend()\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# Another strategy is to visualize the weights that were learned in the first\n",
    "# layer of the network. In most neural networks trained on visual data, the\n",
    "# first layer weights typically show some visible structure when visualized.\n",
    "show_net_weights(net)\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the hyper-parameters over the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **What's wrong?**. Looking at the visualizations above, we see that the loss\n",
    "# is decreasing more or less linearly, which seems to suggest that the learning\n",
    "# rate may be too low. Moreover, there is no gap between the training and\n",
    "# validation accuracy, suggesting that the model we used has low capacity, and\n",
    "# that we should increase its size. On the other hand, with a very large model\n",
    "# we would expect to see more overfitting, which would manifest itself as a\n",
    "# very large gap between the training and validation accuracy.\n",
    "#\n",
    "# **Tuning**. Tuning the hyperparameters and developing intuition for how they\n",
    "# affect the final performance is a large part of using Neural Networks, so we\n",
    "# want you to get a lot of practice. Below, you should experiment with\n",
    "# different values of the various hyperparameters, including hidden layer size,\n",
    "# learning rate, numer of training epochs, and regularization strength. You\n",
    "# might also consider tuning the learning rate decay, but you should be able to\n",
    "# get good performance using the default value.\n",
    "#\n",
    "# **Approximate results**. You should aim to achieve a classification\n",
    "# accuracy of greater than 48% on the validation set. Our best network gets\n",
    "# over 52% on the validation set.\n",
    "#\n",
    "# **Experiment**: You goal in this exercise is to get as good of a result on\n",
    "# CIFAR-10 as you can (52% could serve as a reference), with a fully-connected\n",
    "# Neural Network. Feel free implement your own techniques (e.g. PCA to reduce\n",
    "# dimensionality, or adding dropout, or adding features to the solver, etc.).\n",
    "\n",
    "# **Explain your hyperparameter tuning process in the report.**\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "from utils.tuning import NetGridSearchCV, add_features, apply_pca\n",
    "\n",
    "# Add more features to the input data\n",
    "X_train_enhanced, X_val_enhanced, X_test_enhanced = add_features(X_train, X_val, X_test)\n",
    "\n",
    "print(\"New Training data shape: \", X_train.shape)\n",
    "\n",
    "# Update the input size\n",
    "input_size = X_train_enhanced.shape[1]\n",
    "\n",
    "# Define a suitable hyperparameter grid\n",
    "# param_grid = {\n",
    "#     \"num_iters\": [1000, 2000],\n",
    "#     \"batch_size\": [400, 800],\n",
    "#     \"learning_rate\": [1e-3],\n",
    "#     \"learning_rate_decay\": [0.95],\n",
    "#     \"reg\": [0, 0.25, 0.5],\n",
    "# }\n",
    "\n",
    "# Perform the grid search over the hyperparameters\n",
    "# grid_search = NetGridSearchCV(module_twolayernet.TwoLayerNetv4, param_grid)\n",
    "# grid_search.fit(\n",
    "#     X_train_enhanced,\n",
    "#     y_train,\n",
    "#     X_val_enhanced,\n",
    "#     y_val,\n",
    "#     input_size=input_size,\n",
    "#     hidden_size=hidden_size,\n",
    "#     output_size=num_classes,\n",
    "#     n_jobs=6\n",
    "# )\n",
    "\n",
    "# Get the best hyperparameters and the best model, and train it\n",
    "# best_params = grid_search.get_best_params()\n",
    "\n",
    "# The hyperparamters below are a result of a grid search over the hyperparameters space\n",
    "best_params = {\n",
    "    \"num_iters\": 2000,\n",
    "    \"batch_size\": 800,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"reg\": 0.25,\n",
    "}\n",
    "\n",
    "print(\"Best found hyperparameters: \", best_params)\n",
    "\n",
    "best_net = module_twolayernet.TwoLayerNetv4(\n",
    "    input_size=input_size, hidden_size=hidden_size, output_size=num_classes\n",
    ")\n",
    "stats = best_net.train(\n",
    "    X_train_enhanced,\n",
    "    y_train,\n",
    "    X_val_enhanced,\n",
    "    y_val,\n",
    "    verbose=True,\n",
    "    restore_best_weights=True,\n",
    "    validation_metric=\"val_acc\",\n",
    "    **best_params,\n",
    ")\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "# Predict on the validation set\n",
    "val_acc = (best_net.predict(X_val_enhanced) == y_val).mean()\n",
    "print(f\"\\nValidation accuracy: {val_acc * 100:.2f}\")\n",
    "show_net_weights(best_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you are done experimenting, you should evaluate your final trained\n",
    "# network on the test set; you should get more than 48% accuracy on the test set.\n",
    "test_acc = (best_net.predict(X_test_enhanced) == y_test).mean()\n",
    "print(f'Test accuracy: {test_acc*100:0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comments on Hyperparameter-Tuning**\n",
    "We defined a class `NetGridSearchCV` to perform a grid search over the hyperparameters. The class is initialized with the model class and the hyperparameters grid. We chose the following grid to start with:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    \"num_iters\": [1000, 2000],\n",
    "    \"batch_size\": [400, 800],\n",
    "    \"learning_rate\": [1e-3],\n",
    "    \"learning_rate_decay\": [0.95],\n",
    "    \"reg\": [0, 0.25, 0.5],\n",
    "}\n",
    "```\n",
    "\n",
    "After that we call `grid_search.fit()` with the training and validation sets, hidden size, etc., to start the hyperparameter search and finally, we get the best hyperparameters using `best_params = grid_search.get_best_params()`. We got the following best hyperparameters:\n",
    "\n",
    "```python\n",
    "best_params = {\n",
    "    \"num_iters\": 2000,\n",
    "    \"batch_size\": 800,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"learning_rate_decay\": 0.95,\n",
    "    \"reg\": 0.25,\n",
    "}\n",
    "```\n",
    "Main difference compared to the original ones are the number of iterations, the batch size and the learning rate. For all 3 hyperparameters, we increased the values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4: Implement multi-layer perceptron using PyTorch library (10 points)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have implemented a two-layer network by explicitly writing down the expressions for the forward and backward computations and training algorithms using simple matrix multiplication primitives from the NumPy library.\n",
    "\n",
    "However there are many libraries available designed make experimenting with neural networks faster by abstracting away the details into re-usable modules. One such popular open-source library is PyTorch (https://pytorch.org/). In this final question we will use the PyTorch library to implement the same two-layer network we did before and train it on the Cifar-10 dataset. However, extending a two-layer network to a three or four layered one is a matter of changing two-three lines of code using PyTorch. We will take advantage of this to experiment with deeper networks to improve the performance on the CIFAR-10 classification.\n",
    "\n",
    "To install the pytorch library follow the instruction in\n",
    "https://pytorch.org/get-started/locally/ . If you have access to a Graphics Processing\n",
    "Unit (GPU), you can install the gpu verison and run the exercise on GPU for faster run\n",
    "times. If not, you can install the cpu version (select cuda version None) and run on the\n",
    "cpu. Having gpu access is not necessary to complete the exercise.  There are good tutorials\n",
    "for getting started with pytorch on their website (https://pytorch.org/tutorials/).\n",
    "\n",
    "a) Complete the code to implement a multi-layer perceptron network in the class\n",
    "`MultiLayerPerceptron`. This includes instantiating the\n",
    "required layers from `torch.nn` and writing the code for forward pass. Initially you \n",
    "should write the code for the same two-layer network we have seen before.\n",
    "(3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.normal_(0.0, 1e-3)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "def update_lr(optimizer, lr):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "# --------------------------------\n",
    "# Device configuration\n",
    "# --------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: %s\" % device)\n",
    "\n",
    "# --------------------------------\n",
    "# Hyper-parameters\n",
    "# --------------------------------\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = [1024, 512, 512]\n",
    "num_classes = 10\n",
    "num_epochs = 20\n",
    "batch_size = 200\n",
    "learning_rate = 1e-3\n",
    "learning_rate_decay = 0.95\n",
    "reg=0.001\n",
    "num_training= 49000\n",
    "num_validation =1000\n",
    "train = True\n",
    "drop_prob=0.3\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Load the CIFAR-10 dataset\n",
    "\n",
    "# This time we rely on torchvision.datasets classes, as they have already provide implementation\n",
    "# for many datasets.\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Create a transform object, which pre-processes every sample before returning.\n",
    "norm_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    ")\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/exercise-2/\",\n",
    "    train=True,\n",
    "    transform=norm_transform,\n",
    "    download=False,  # Since we already downloaded it in the previous questions\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(\n",
    "    root=\"./data/exercise-2/\", train=False, transform=norm_transform\n",
    ")\n",
    "# -------------------------------------------------\n",
    "# Prepare the training and validation splits\n",
    "# We use the Subset wrapper dataset, which takes a dataset object and certain indices\n",
    "# and returns a new Dataset object of only those specific samples.\n",
    "# -------------------------------------------------\n",
    "mask = list(range(num_training))\n",
    "train_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "val_dataset = torch.utils.data.Subset(cifar_dataset, mask)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Data loader\n",
    "# Data Loader takes a dataset object and returns samples in batches.\n",
    "# It's a Generator and hence we can simply apply a for-loop over it.\n",
    "# You can also ask it to create batches of specific size and configure it\n",
    "# whether to shuffle the data (useful for training).\n",
    "# -------------------------------------------------\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please explain why the following two lengths are different and how they relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader), len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train_dataset variable represents the entire dataset used for training. It contains all the data samples available for the training process (in this case, 49000). On the other hand, the train_loader variable represents a collection of batches, where each batch is a subset of the train_dataset and has size BATCH_SIZE. So, to find the length of train_loader, we need to perform the following: len(train_dataset) / BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================\n",
    "# Q4: Implementing multi-layer perceptron in PyTorch\n",
    "#======================================================================================\n",
    "# So far we have implemented a two-layer network using numpy by explicitly\n",
    "# writing down the forward computation and deriving and implementing the\n",
    "# equations for backward computation. This process can be tedious to extend to\n",
    "# large network architectures\n",
    "#\n",
    "# Popular deep-learining libraries like PyTorch and Tensorflow allow us to\n",
    "# quickly implement complicated neural network architectures. They provide\n",
    "# pre-defined layers which can be used as building blocks to define our\n",
    "# network. They also enable automatic-differentiation, which allows us to\n",
    "# define only the forward pass and let the libraries perform back-propagation\n",
    "# using automatic differentiation.\n",
    "#\n",
    "# In this question we will implement a multi-layer perceptron using the PyTorch\n",
    "# library.  Please complete the code for the MultiLayerPerceptron, training and\n",
    "# evaluating the model. Once you can train the two layer model, experiment with\n",
    "# adding more layers and\n",
    "#--------------------------------------------------------------------------------------\n",
    "\n",
    "#-------------------------------------------------\n",
    "# Fully connected neural network with one hidden layer\n",
    "#-------------------------------------------------\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers, num_classes, drop_prob, use_batch_norm=False):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        layers = [nn.Flatten()]\n",
    "        for hidden_layer_size in hidden_layers:\n",
    "            layers.append(nn.Linear(input_size, hidden_layer_size))\n",
    "\n",
    "            if use_batch_norm:\n",
    "                layers.append(nn.BatchNorm1d(hidden_layer_size))\n",
    "            \n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "            if drop_prob > 0:\n",
    "                layers.append(nn.Dropout(drop_prob))\n",
    "                \n",
    "            input_size = hidden_layer_size\n",
    "\n",
    "        layers.append(nn.Linear(input_size, num_classes))\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Sequential module takes many modules as argument and \n",
    "        # at forward pass propagates the data through all of them\n",
    "        self.layers = nn.Sequential(*layers) \n",
    "\n",
    "    def forward(self, x):\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "        return self.layers(x)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "model = MultiLayerPerceptron(input_size, hidden_size, num_classes, drop_prob=drop_prob, use_batch_norm=True).to(device)\n",
    "print(model)\n",
    "\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters: {num_trainable_params}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Complete the code to train the network. Make use of the loss function `torch.nn.CrossEntropyLoss` to compute the loss and `loss.backward()` to compute the gradients. Once gradients are computed, `optimizer.step()` can be invoked to update the model. Your should be able to achieve similar performance ($>$ 48\\% accuracy on the test set) as in Q3. Report the final test accuracy you achieve with a two-layer network. (3 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "model.apply(weights_init)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), amsgrad=True) # lr=learning_rate, weight_decay=reg)\n",
    "\n",
    "# Train the model\n",
    "lr = learning_rate\n",
    "total_step = len(train_loader)\n",
    "\n",
    "# For plotting\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# For early stopping\n",
    "patience = 3\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "enabled_early_stopping = False\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    curr_loss = 0\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # Using https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-optim\n",
    "        # as a reference\n",
    "        model.train()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(images)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(logits, labels)\n",
    "        curr_loss += loss.item()\n",
    "\n",
    "        # Zero the gradients before running the backward pass.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its parameters        \n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute predicted values\n",
    "        predicted = logits.argmax(axis=1)\n",
    "        \n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}\",\n",
    "                end=\"\\r\",\n",
    "            )\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    training_losses.append(curr_loss / len(train_loader))\n",
    "\n",
    "    print(f\"\\nTrain accuracy is: {(100 * correct / total) : 0.2f} %\")\n",
    "\n",
    "    # Code to update the lr\n",
    "    # lr *= learning_rate_decay\n",
    "    # update_lr(optimizer, lr)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        curr_loss = 0\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "           \n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "           \n",
    "            model.eval()\n",
    "            logits = model(images)\n",
    "            predicted = logits.argmax(axis=1)\n",
    "            loss = criterion(logits, labels)\n",
    "            curr_loss += loss.item()\n",
    "           \n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f\"Validation accuracy is: {(100 * correct / total):0.2f} %\")\n",
    "        validation_losses.append(curr_loss / len(val_loader))\n",
    "\n",
    "        if enabled_early_stopping:\n",
    "            # Early stopping check\n",
    "            if validation_losses[-1] < best_val_loss:\n",
    "                best_val_loss = validation_losses[-1]\n",
    "                patience_counter = 0\n",
    "                # Save the best model\n",
    "                torch.save(model.state_dict(), f\"model_best_{hidden_size}.ckpt\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), f\"model_{hidden_size}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the test code once you have your by setting train flag to false\n",
    "# and loading the best model\n",
    "best_model = None  # torch.load()\n",
    "best_model_state_dict = torch.load(\n",
    "    f\"model_{hidden_size}.ckpt\", map_location=\"cpu\" if not torch.cuda.is_available() else device\n",
    ")\n",
    "model.load_state_dict(best_model_state_dict)\n",
    "model.to(device)\n",
    "\n",
    "# Test the model\n",
    "model.eval()\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        ####################################################\n",
    "        # TODO: Implement the evaluation code              #\n",
    "        # 1. Pass the images to the model                  #\n",
    "        # 2. Get the most confident predicted class        #\n",
    "        ####################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        logits = model(images)\n",
    "        predicted = logits.argmax(axis=1)\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if total == 1000:\n",
    "            break\n",
    "\n",
    "    print(f\"Accuracy of the network on the {total} test images: {(100 * correct / total):0.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance report on a simple two-layer network**\n",
    "\n",
    "The two-layered Pytorch model implementation: \n",
    "```python\n",
    "MultiLayerPerceptron(\n",
    "  (layers): Sequential(\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "    (1): Linear(in_features=3072, out_features=50, bias=True)\n",
    "    (2): ReLU()\n",
    "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "Number of trainable parameters: 154160\n",
    "```\n",
    "\n",
    "achieved an accuracy of **58.57%** on training, **53.60%** on validation and **51.60%** on the test dataset. It is worth mentioning that dropout was not used in either the Pytorch or feedforward network implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance report on different model configurations**\n",
    "\n",
    "With a fixed starting `learning_rate = 1.00E-03`, we experimented with different model configurations, as shown below:\n",
    "\n",
    "| Layer Configuration         | Training Accuracy | Validation Accuracy | Test Accuracy | Number of Params |\n",
    "| :-------------------------: | :---------------: | :-----------------: | :-----------: | :--------------: |\n",
    "| [50]                        | 58.57             | 53.6                | 51.6          | 154160           |\n",
    "| [50, 256]                   | 63.1              | 53.4                | 54.7          | 169276           |\n",
    "| [50, 256, 128]              | 64.08             | 54.6                | 54.8          | 200892           |\n",
    "| [50, 256, 128, 64]          | 9.97              | 7.9                 | 10            | 208508           |\n",
    "| [50, 256, 128, 64, 64]      | 9.91              | 7.8                 | 9             | 212669           |\n",
    "\n",
    "We noticed that we gained a slight improvement in the test accuracy by adding more layers to the network. However, the model with 4 and 5 layers performed poorly. This could be due to the vanishing gradient problem, which is common in deeper networks (specially if we are setting the learning rate ourselves). The model with 3 layers performed the best, with a test accuracy of **54.8%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance report on a more complex network**\n",
    "\n",
    "The Pytorch model implementation: \n",
    "\n",
    "```python\n",
    "MultiLayerPerceptron(\n",
    "  (layers): Sequential(\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "    (1): Linear(in_features=3072, out_features=512, bias=True)\n",
    "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (3): ReLU()\n",
    "    (4): Dropout(p=0.3, inplace=False)\n",
    "    (5): Linear(in_features=512, out_features=256, bias=True)\n",
    "    (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (7): ReLU()\n",
    "    (8): Dropout(p=0.3, inplace=False)\n",
    "    (9): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (10): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (11): ReLU()\n",
    "    (12): Dropout(p=0.3, inplace=False)\n",
    "    (13): Linear(in_features=128, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "Number of trainable parameters: 1740682\n",
    "```\n",
    "achieved an accuracy of **67.43%** on training, **59.30%** on validation and **56.90%** on the test dataset which is better than the two-layer network. The model is deeper and has more parameters than the two-layer network. The model also uses batch normalization and dropout to improve the performance. \n",
    "\n",
    "The training and validation loss plot for the best model is shown below (we can see that the model is not overfitting):\n",
    "\n",
    "![Train-Validation Loss plot for best model](./data/exercise-2/train_val_loss_plot_1.png)\n",
    "\n",
    "**Note:** For this model and the ones below, we used the following optimizer definition: `optimizer = torch.optim.Adam(model.parameters(), amsgrad=True)`, that removes the learning rate and weight decay manual tuning, and uses the AMSGrad variant of the Adam algorithm from the paper [On the Convergence of Adam and Beyond](https://openreview.net/forum?id=ryQu7f-RZ). \n",
    "\n",
    "**Note 2:** \"Iterations\" is actually the number of epochs in this plot and the following.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance report on an even more complex network**\n",
    "\n",
    "The Pytorch model implementation: \n",
    "\n",
    "```python\n",
    "MultiLayerPerceptron(\n",
    "  (layers): Sequential(\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "    (1): Linear(in_features=3072, out_features=1024, bias=True)\n",
    "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (3): ReLU()\n",
    "    (4): Dropout(p=0.3, inplace=False)\n",
    "    (5): Linear(in_features=1024, out_features=512, bias=True)\n",
    "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (7): ReLU()\n",
    "    (8): Dropout(p=0.3, inplace=False)\n",
    "    (9): Linear(in_features=512, out_features=256, bias=True)\n",
    "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (11): ReLU()\n",
    "    (12): Dropout(p=0.3, inplace=False)\n",
    "    (13): Linear(in_features=256, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "Number of trainable parameters: 3809034\n",
    "```\n",
    "achieved an accuracy of **71.43%** on training, **59.80%** on validation and **59.40%** on the test dataset. This was trained under the same conditions as the previous model, but it had double the number of neurons per each layer.\n",
    "\n",
    "The training and validation loss plot for this model is shown below (we can see that the model is starting to overfit):\n",
    "\n",
    "![Train-Validation Loss plot for starting-to-overfit model](./data/exercise-2/train_val_loss_plot_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Performance report on an even more complex network**\n",
    "\n",
    "The Pytorch model implementation: \n",
    "```python\n",
    "MultiLayerPerceptron(\n",
    "  (layers): Sequential(\n",
    "    (0): Flatten(start_dim=1, end_dim=-1)\n",
    "    (1): Linear(in_features=3072, out_features=1024, bias=True)\n",
    "    (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (3): ReLU()\n",
    "    (4): Dropout(p=0.3, inplace=False)\n",
    "    (5): Linear(in_features=1024, out_features=512, bias=True)\n",
    "    (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (7): ReLU()\n",
    "    (8): Dropout(p=0.3, inplace=False)\n",
    "    (9): Linear(in_features=512, out_features=256, bias=True)\n",
    "    (10): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (11): ReLU()\n",
    "    (12): Dropout(p=0.3, inplace=False)\n",
    "    (13): Linear(in_features=256, out_features=128, bias=True)\n",
    "    (14): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (15): ReLU()\n",
    "    (16): Dropout(p=0.3, inplace=False)\n",
    "    (17): Linear(in_features=128, out_features=10, bias=True)\n",
    "  )\n",
    ")\n",
    "Number of trainable parameters: 3840906\n",
    "```\n",
    "achieved an accuracy of **84.20%** on training, **59.30%** on validation and **60.40%** on the test dataset. This was trained under the same conditions as the previous model, but it had an additional layer with 128 neurons.\n",
    "\n",
    "The training and validation loss plot for this model is shown below (we can see that the model is *clearly* overfitting):\n",
    "\n",
    "![Train-Validation Loss plot for overfitted model](./data/exercise-2/train_val_loss_plot_3.png)\n",
    "\n",
    "Since we got the best performance in training and very good validation accuracy, we implemented Early Stopping (with a patience of 3) on this configuration to avoid overfitting. However, the test accuracy did not improve compared to the best model without Early Stopping. It achieved a test accuracy of **64.11%**, **60.40%** on validation and **59.70%** on the test dataset. Note however that usually we don't have access to the test set, so we would not know the test accuracy and would have to rely on the validation accuracy to choose the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of the results**\n",
    "The results of our experiments are summarized in the graph below. \n",
    "\n",
    "| Method | Description                                                  |\n",
    "|--------|--------------------------------------------------------------|\n",
    "| M1     | Simple two-layer network                                     |\n",
    "| M2     | Simple four-layer network                                    |\n",
    "| M3     | Four layer network with Batch Norm and Dropout               |\n",
    "| M4     | **M3** with doubled the number of neurons per hidden layer   |\n",
    "| M5     | **M4** with an additional hidden layer of size 128           |\n",
    "| M6     | **M5** and Early Stopping                                    |\n",
    "\n",
    "Here is the summary of the results:\n",
    "\n",
    "![Summary of the results](./data/exercise-2/summary.png)\n",
    "\n",
    "Which clearly shows that the M4 configuration is the best one with regards to the trade-off between training and validation accuracy, i.e, avoiding overfitting. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
