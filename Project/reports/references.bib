@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})

@inproceedings{takuya1999realtime,
  title     = {Realtime chord recognition of musical sound: Asystem using common lisp music},
  author    = {Takuya, Fujishima},
  booktitle = {Proceedings of the International Computer Music Conference 1999, Beijing},
  year      = {1999}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}


@article{math11081915,
  author         = {Li, Shuyu and Sung, Yunsick},
  title          = {MelodyDiffusion: Chord-Conditioned Melody Generation Using a Transformer-Based Diffusion Model},
  journal        = {Mathematics},
  volume         = {11},
  year           = {2023},
  number         = {8},
  article-number = {1915},
  url            = {https://www.mdpi.com/2227-7390/11/8/1915},
  issn           = {2227-7390},
  abstract       = {Artificial intelligence, particularly machine learning, has begun to permeate various real-world applications and is continually being explored in automatic music generation. The approaches to music generation can be broadly divided into two categories: rule-based and data-driven methods. Rule-based approaches rely on substantial prior knowledge and may struggle to handle large datasets, whereas data-driven approaches can solve these problems and have become increasingly popular. However, data-driven approaches still face challenges such as the difficulty of considering long-distance dependencies when handling discrete-sequence data and convergence during model training. Although the diffusion model has been introduced as a generative model to solve the convergence problem in generative adversarial networks, it has not yet been applied to discrete-sequence data. This paper proposes a transformer-based diffusion model known as MelodyDiffusion to handle discrete musical data and realize chord-conditioned melody generation. MelodyDiffusion replaces the U-nets used in traditional diffusion models with transformers to consider the long-distance dependencies using attention and parallel mechanisms. Moreover, a transformer-based encoder is designed to extract contextual information from chords as a condition to guide melody generation. MelodyDiffusion can automatically generate diverse melodies based on the provided chords in practical applications. The evaluation experiments, in which Hits@k was used as a metric to evaluate the restored melodies, demonstrate that the large-scale version of MelodyDiffusion achieves an accuracy of 72.41% (k = 1).},
  doi            = {10.3390/math11081915}
}


@article{wang2024yolov9,
  title   = {Yolov9: Learning what you want to learn using programmable gradient information},
  author  = {Wang, Chien-Yao and Yeh, I-Hau and Liao, Hong-Yuan Mark},
  journal = {arXiv preprint arXiv:2402.13616},
  year    = {2024}
}

@article{su2020audeo,
  title   = {Audeo: Audio generation for a silent performance video},
  author  = {Su, Kun and Liu, Xiulong and Shlizerman, Eli},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {3325--3337},
  year    = {2020}
}

@inproceedings{stark2009real,
  title     = {Real-time chord recognition for live performance},
  author    = {Stark, Adam M and Plumbley, Mark D},
  booktitle = {ICMC},
  year      = {2009}
}

@article{korzeniowski2016feature,
  title   = {Feature learning for chord recognition: The deep chroma extractor},
  author  = {Korzeniowski, Filip and Widmer, Gerhard},
  journal = {arXiv preprint arXiv:1612.05065},
  year    = {2016}
}

@inproceedings{boulanger2013audio,
  title        = {Audio Chord Recognition with Recurrent Neural Networks.},
  author       = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
  booktitle    = {ISMIR},
  pages        = {335--340},
  year         = {2013},
  organization = {Curitiba}
}

@article{Kristian_Zaman_Tenoyo_Jodhinata_2024,
  title        = {Advancing Guitar Chord Recognition: A Visual Method Based on Deep Convolutional Neural Networks and Deep Transfer Learning},
  volume       = {18},
  url          = {https://ph01.tci-thaijo.org/index.php/ecticit/article/view/254624},
  doi          = {10.37936/ecti-cit.2024182.254624},
  abstractnote = {&amp;lt;p&amp;gt;Initiated in 1999, Automatic Chord Recognition (ACR) primarily relied on audio data, facing challenges, especially with high timbre sounds, which led to a shift towards visual methods for recognizing guitar chords due to their distinct hand configurations. This project explores visual guitar chord identification, harnessing fretboard features, including hand arrangements and positions. It also investigates the limited advantages of transfer learning due to the absence of pertinent pre-trained weights. The developed model employs deep learning, DCNN methodologies, and techniques like normalization, attening, and dropout to identify 14 major and minor keys without fret position restrictions. Enhanced by data augmentation, a self-compiled dataset of over 13,000 samples from 10 contributors effectively trains the model for new data. After testing 115 new examples, the system achieved an 83% accuracy on both live and pre-recorded video data. These results demonstrate the feasibility of employing a deep convolutional neural network (DCNN) focused visual approach for guitar chord identification. Furthermore, this study suggests exciting potential for future advancements in the Music Information Retrieval (MIR) field.&amp;lt;/p&amp;gt;},
  number       = {2},
  journal      = {ECTI Transactions on Computer and Information Technology (ECTI-CIT)},
  author       = {Kristian, Yosi and Zaman, Lukman and Tenoyo, Michael and Jodhinata, Andreas},
  year         = {2024},
  month        = {May},
  pages        = {235-249}
}

@inproceedings{du2023conditional,
  title     = {Conditional Generation of Audio from Video via Foley Analogies},
  author    = {Du, Yuexi and Chen, Ziyang and Salamon, Justin and Russell, Bryan and Owens, Andrew},
  booktitle = {Conference on Computer Vision and Pattern Recognition 2023},
  year      = {2023}
}

@misc{guitar-chords-daewp_dataset,
  title        = { Guitar Chords Dataset },
  type         = { Open Source Dataset },
  author       = { joaomarcoscrs },
  howpublished = { \url{ https://universe.roboflow.com/joaomarcoscrs/guitar-chords-daewp } },
  url          = { https://universe.roboflow.com/joaomarcoscrs/guitar-chords-daewp },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { jun },
  note         = { visited on 2024-06-29 }
}

@misc{guitar-chord-tvon8_dataset,
  title        = { Guitar Chord Dataset },
  type         = { Open Source Dataset },
  author       = { My Work },
  howpublished = { \url{ https://universe.roboflow.com/my-work-3idwy/guitar-chord-tvon8 } },
  url          = { https://universe.roboflow.com/my-work-3idwy/guitar-chord-tvon8 },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { may },
  note         = { visited on 2024-06-29 }
}

@misc{guitar-ppfil_dataset,
  title        = { Guitar Dataset },
  type         = { Open Source Dataset },
  author       = { SOEN357 },
  howpublished = { \url{ https://universe.roboflow.com/soen357/guitar-ppfil } },
  url          = { https://universe.roboflow.com/soen357/guitar-ppfil },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { may },
  note         = { visited on 2024-06-29 }
}

@misc{done-npcll_dataset,
  title        = { Done Dataset },
  type         = { Open Source Dataset },
  author       = { Done },
  howpublished = { \url{ https://universe.roboflow.com/done-ygt9y/done-npcll } },
  url          = { https://universe.roboflow.com/done-ygt9y/done-npcll },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { apr },
  note         = { visited on 2024-06-29 }
}

@misc{guitar-chord-bounding-box_dataset,
  title        = { Guitar Chord Bounding box Dataset },
  type         = { Open Source Dataset },
  author       = { GRC MASK custom dataset },
  howpublished = { \url{ https://universe.roboflow.com/grc-mask-custom-dataset/guitar-chord-bounding-box } },
  url          = { https://universe.roboflow.com/grc-mask-custom-dataset/guitar-chord-bounding-box },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { jun },
  note         = { visited on 2024-06-29 }
}

@misc{guitar-chord-handshape_dataset,
  title        = { Guitar Chord HandShape Dataset },
  type         = { Open Source Dataset },
  author       = { GRC MASK custom dataset },
  howpublished = { \url{ https://universe.roboflow.com/grc-mask-custom-dataset/guitar-chord-handshape } },
  url          = { https://universe.roboflow.com/grc-mask-custom-dataset/guitar-chord-handshape },
  journal      = { Roboflow Universe },
  publisher    = { Roboflow },
  year         = { 2024 },
  month        = { apr },
  note         = { visited on 2024-06-29 }
}

@inproceedings{Xi2018,
  author    = {Q. Xi and R. Bittner and J. Pauwels and X. Ye and J. P. Bello},
  title     = {Guitarset: A Dataset for Guitar Transcription},
  booktitle = {19th International Society for Music Information Retrieval Conference (ISMIR)},
  year      = {2018},
  address   = {Paris, France},
  month     = {September}
}

@inproceedings{Bambach_2015_ICCV,
  author    = {Bambach, Sven and Lee, Stefan and Crandall, David J. and Yu, Chen},
  title     = {Lending A Hand: Detecting Hands and Recognizing Activities in Complex Egocentric Interactions},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month     = {December},
  year      = {2015}
}

@misc{russakovsky2015imagenetlargescalevisual,
  title         = {ImageNet Large Scale Visual Recognition Challenge},
  author        = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
  year          = {2015},
  eprint        = {1409.0575},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1409.0575}
}

@misc{lin2015microsoftcococommonobjects,
  title         = {Microsoft COCO: Common Objects in Context},
  author        = {Tsung-Yi Lin and Michael Maire and Serge Belongie and Lubomir Bourdev and Ross Girshick and James Hays and Pietro Perona and Deva Ramanan and C. Lawrence Zitnick and Piotr Doll√°r},
  year          = {2015},
  eprint        = {1405.0312},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/1405.0312}
}

@article{Jadhav_transferlearning,
  author  = {Jadhav, Yogesh and Patel, Ashish and Jhaveri, Rutvij and Raut, Roshani},
  year    = {2022},
  month   = {08},
  pages   = {},
  title   = {Transfer Learning for Audio Waveform to Guitar Chord Spectrograms Using the Convolution Neural Network},
  volume  = {2022},
  journal = {Mobile Information Systems},
  doi     = {10.1155/2022/8544765}
}

@inproceedings{sandler2018mobilenetv2,
  title     = {Mobilenetv2: Inverted residuals and linear bottlenecks},
  author    = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {4510--4520},
  year      = {2018}
}

@article{howard2017mobilenets,
  title   = {Mobilenets: Efficient convolutional neural networks for mobile vision applications},
  author  = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal = {arXiv preprint arXiv:1704.04861},
  year    = {2017}
}

@misc{peft,
  title        = {PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods},
  author       = {Sourab Mangrulkar and Sylvain Gugger and Lysandre Debut and Younes Belkada and Sayak Paul and Benjamin Bossan},
  howpublished = {\url{https://github.com/huggingface/peft}},
  year         = {2022}
}

@misc{Extendin94:online,
  author       = {},
  title        = {Extending YOLOv8 Model With New Classes Without Affecting Old Weights},
  howpublished = {\url{https://y-t-g.github.io/tutorials/yolov8n-add-classes/}},
  month        = {},
  year         = {},
  note         = {(Accessed on 07/19/2024)}
}

@article{zhang2020mediapipe,
  title={Mediapipe hands: On-device real-time hand tracking},
  author={Zhang, Fan and Bazarevsky, Valentin and Vakunov, Andrey and Tkachenka, Andrei and Sung, George and Chang, Chuo-Ling and Grundmann, Matthias},
  journal={arXiv preprint arXiv:2006.10214},
  year={2020}
}

@article{oquab2023dinov2,
  title={Dinov2: Learning robust visual features without supervision},
  author={Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}

@inproceedings{ooaku2018guitar,
  title={Guitar chord recognition based on finger patterns with deep learning},
  author={Ooaku, Takumi and Linh, Tran Duy and Arai, Masayuki and Maekawa, Tsukasa and Mizutani, Kozo},
  booktitle={Proceedings of the 4th International Conference on Communication and Information Processing},
  pages={54--57},
  year={2018}
}

@inproceedings{tran2019cnn,
  title={CNN Transfer Learning for Visual Guitar Chord Classification},
  author={Tran, Leon and Zhang, Shawn and Zhou, Eric},
  booktitle={30th Conference on Neural Information Processing Systems},
  year={2019}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{redmon2016you,
  title={You only look once: Unified, real-time object detection},
  author={Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={779--788},
  year={2016}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@inproceedings{ho1995random,
  title={Random decision forests},
  author={Ho, Tin Kam},
  booktitle={Proceedings of 3rd international conference on document analysis and recognition},
  volume={1},
  pages={278--282},
  year={1995},
  organization={IEEE}
}

@article{ren2016faster,
  title={Faster R-CNN: Towards real-time object detection with region proposal networks},
  author={Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={6},
  pages={1137--1149},
  year={2016},
  publisher={IEEE}
}