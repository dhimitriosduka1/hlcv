% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

\newcommand{\dhimitrios}[1]{\textcolor{red}{Dhimitrios: #1}}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Team \#16: From Strings to Sequences --- Classifying and Generating Music from Acoustic Guitar Notes}


\author{
    Camilo Martínez\\
    7057573\\
    \and
    Dhimitrios Duka\\
    7059153\\
    \and
    Honglu Ma\\
    7055053\\
}
\maketitle

%%%%%%%%% BODY TEXT
\section{Abstract}
% Should be inside \emph{} https://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Guarnera_On_the_Exploitation_of_Deepfake_Model_Recognition_CVPRW_2022_paper.pdf
\emph{First introduced in the late 1999s by \cite{takuya1999realtime}, Automatic Chord Recognition (ACR) is an information retrieval task that automatically recognizes the chords played in a music piece. During the last decades, many approaches—ranging from aural\footnote{Related to sense of hearing.} to visual-based methods—have been proposed to tackle this task. In this work, we explore a Computer Vision based approach, which aims to perform ACR based on hand patterns. We extend the work by \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024} by exploring the potential of using state-of-the-art deep learning models and techniques with an additional proposal for an audio generation module as future work.}

\section{Introduction}
ACR is an information retrieval task that automatically recognizes the chords played in a music piece, whether it be an audio or video file. The ability to accurately recognize and identify chords is crucial for various downstream applications such as music analysis, music transcription, or even restoration of corrupted musical performances.

Our work aims to improve ACR in the context of acoustic guitars. We base our work on \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024} and extend it by exploring the YOLO \cite{redmon2016you} and Faster R-CNN \cite{ren2016faster} family for fretboard\footnote{The neck of the guitar.} recognition, alongside ViT \cite{dosovitskiy2020image} and DINOv2 \cite{oquab2023dinov2} architectures for chord recognition.

% Additionally, we extend this work by implementing a chord-to-audio generation module, enabling the generation of audio directly from recognized chord labels.

\section{Related Work}
Over recent decades, many different approaches have been proposed to tackle the ACR task. The first ACR system was introduced in the late 1999s by \cite{takuya1999realtime}, where LISP music was utilized to perform chord recognition at the signal level. With the rise of Computer Vision, researchers began exploring the potential of visual-based approaches to tackle the ACR task making use of hand patterns to perform chord classification. This approach was heavily based on the fact that humans often find it easier to recognize chords based on visual cues rather than auditory ones. \cite{su2020audeo} was able to successfully classify chords being played on a piano and produce sound; \cite{tran2019cnn} and \cite{ooaku2018guitar} replicated this idea on acoustic guitars.

Inspired by the previous works, \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024} employed a Single Shot Detection (SSD) model undergirded by a MobileNetV2 base  model \cite{sandler2018mobilenetv2},  pre-trained on the EgoHands \cite{Bambach_2015_ICCV}, for the fretboard detection and further refined the bounding box with a DCNN\footnote{Deep Convolutional Neural Network.} model. For chord classification, they used an InceptionResnetV2 with additional CNN layers. In contrast to previous works, they used the entire fretboard as input to the model, allowing it to learn the spatial relationships between the chords and frets achieving state-of-the-art performance.

\section{Datasets}
We identified a significant gap in available datasets for the task of guitar chord recognition. Initially, we created our own by recording 90-second videos for each chord in three different environments, ensuring high quality by capturing them in 4K resolution at 60 fps. We extracted the frames from the video and downsampled them to a resolution of 640 $\times$ 360 pixels. This process generated approximately 30,000 frames per chord. To increase the diversity of the dataset, we used two different sampling methods: simple random sampling and kNN-based sampling. In the former method, we selected 1,000 frames at random, while in the latter, we used the kNN to choose 1,000 frames that were the most distinct from one another.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.48\textwidth]{images/final/Guitar-chords-ours_t-sne_plot.png}
    \caption{The t-SNE plot of our dataset containing 14 chords. Each point represents a KNN-sampled frame, with the color indicating the corresponding chord label.}
    \label{fig:ours-tsne-plot}
\end{figure}

Unfortunately, both sampling strategies resulted in an overly simplistic dataset that failed to capture the real-world complexity of chords, as shown by Figure \ref{fig:ours-tsne-plot}. This resulted in poor model generalization. However, rather than abandoning our dataset we used it as a test set to evaluate the generalizability of our model. In the end, we decided to use existing datasets \cite{guitar-chord-tvon8_dataset,guitar-chord-bounding-box_dataset, guitar-chord-handshape_dataset, guitar-chords-daewp_dataset} for training the models, merging them to create a more complex dataset which resulted in significantly better results.

This change in our approach necessitated a change in the scope of our chord recognition task. As a consequence of using existing datasets, we were limited to only seven chords in total—A, B, C, D, E, F, and G—down from the 14 chords originally planned.

For the fretboard detection task, we used pre-trained versions of the models on the COCO dataset\footnote{A dataset of considerable size commonly dedicated to object identification, where pproximately 200,000 labeled images are organized into 80 distinct categories \cite{lin2015microsoftcococommonobjects}. Although somewhat comparable to ImageNet, the COCO dataset possesses a distinct emphasis.}; this is further explained in Sec. \ref{sec:methods}. To finetune it, we used the following dataset publicly available in \href{https://universe.roboflow.com/}{Roboflow}: \cite{guitar-necks-detector}.

\section{Methods}\label{sec:methods}
In the following sections, we will give a overview of the methods used in our work for performing fretboard detection and guitar chord classification.

\subsection{Fretboard Detection}
We experimented with different models from the YOLO \cite{redmon2016you} and Faster R-CNN family \cite{ren2016faster}. We tried two different fine-tuning methods: freezing every layer except the last one and not freezing any layer, that is, finetuning the whole model. Both methods are fundamentally different and serve different purposes. The first method is used to finetune the model to a specific task while keeping the backbone as it is. This allows us to keep the previous learned features and classes. On the other hand, the second method will finetune the
whole model to the new task, while forgetting some of the previous learned features and classes. That is, the latter will have 2 classes after finetuning, the fretboard and the background.

\dhimitrios{Do we need to add more here?}

\subsection{Guitar Chord Classification}
We used two different approaches for guitar chord classification: a hand pose estimation + classifier approach and a classifier only approach.

\subsubsection{Hand Pose Estimation + Classifier}
First, we wanted to try a simple yet interesting approach. For a given sample image, we utilized a hand pose estimation model to extract the hand shape from it, which was then used as the input to a classifier. We used MediaPipe to extract the hand shape followed by different classifiers—SVM \cite{cortes1995support}, Random Forest \cite{ho1995random}, and a simple MLP—to classify the chords.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/final/hand_pose_estimation_classifier.png}
%     \caption{The pipeline of the Hand Pose Estimation + Classifier approach. First, the image is passed through a hand pose estimation model to extract the landmarks. Then the result is passed through a classifier to determine the chord being played.}
%     \label{fig:hand-pose-estimation-classifier}
% \end{figure}

% We used MediaPipe to extract the hand shape followed by different classifiers—SVM \cite{cortes1995support}, Random Forest \cite{ho1995random}, and a simple MLP—to classify the chords.

\subsubsection{Classifier only approach}
Next, we wanted to explore the potential benefits of using more advanced architectures to perform chord classification. We decided to experiment with Vision Transformers (ViT) \cite{dosovitskiy2020image}, specifically ViT-B/16, ViT-B/32, ViT-L/16, and ViT-L/32, to assess how different configurations of patch sizes and model sizes would impact performance. Additionally, we were also interested in evaluating the effectiveness of pre-trained self-supervised models in our task, so we also included DINOv2 \cite{oquab2023dinov2} in our experiments. This allowed us to compare their performance against the ViT models and explore whether self-supervised learning offers advantages in this task.

\subsection{Audio Generation}
For the audio generation module, we employed a straightforward approach. First, we put the generated chords from the classifer into 1 second buckets. That is, if the video is recorded at 30 frames per second, we would group the chords in 30 frames buckets. Then, we found the most prominent chord in each bucket and use that as the predicted chord. This in turn also served as a way to smooth out the predictions. Finally, we used pre-recorded audio samples of each chord and concatenated them to produce the sound that was being played.

\section{Experimental Results and Analyses}
\label{sec:results}

In the following sections, we will present the results of our experiments and provide an analysis of the performance of the models used in our work.

\subsection{Fretboard Detection}

\begin{table}[thb]
    \scriptsize
    \centering
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Model}               & \textbf{P}      & \textbf{R}      & \textbf{mAP50-95} & \textbf{mAP50}  \\
        \midrule
        YOLOv8                       & \textbf{98.9\%} & 93.0\%          & \textbf{88.7\%}   & \textbf{98.2\%} \\
        YOLOv9                       & 96.4\%          & \textbf{96.8\%} & 85.3\%            & 97.8\%          \\
        YOLOv10                      & 94.2\%          & 87.0\%          & 80.0\%            & 94.4\%          \\
        Faster-RCNN-Resnet50         & 80.8\%          & 82.4\%          & 77.5\%            & 94.0\%          \\
        Faster-RCNN-MobileNetv3      & 79.4\%          & 81.6\%          & 75.7\%            & 94.9\%          \\
        YOLOv8 (FB)                  & 76.7\%          & \textbf{85.1\%} & 53.4\%            & 87.8\%          \\
        YOLOv9 (FB)                  & \textbf{82.4\%} & 74.7\%          & 54.7\%            & 87.0\%          \\
        YOLOv10 (FB)                 & 81.4\%          & 84.0\%          & \textbf{71.2\%}   & 89.9\%          \\
        Faster-RCNN-Resnet50 (FB)    & 62.9\%          & 66.3\%          & 59.0\%            & \textbf{93.4\%} \\
        Faster-RCNN-MobileNetv3 (FB) & 71.7\%          & 73.6\%          & 68.3\%            & 93.0\%          \\
        \bottomrule
    \end{tabular}
    \caption{Performance metrics of different models on the evaluation dataset, shown in percentages. Each column represents a specific metric: Precision, Recall, mAP50-95, and mAP50. (FB) denotes models finetuned with a Frozen Backbone.}
    \label{tab:metrics-results}
\end{table}



\begin{figure}[thb]
    \centering
    \includegraphics[width=\columnwidth]{images/final/recall_vs_map50.pdf}
    \caption{Recall vs. mAP@50 for the models tested and finetuned on the \emph{fretboard} class.}
    \label{fig:fretboard-models-recall-map}
\end{figure}

\dhimitrios{Change this a bit}
Since our YOLOv9 model did not lose its capability to detect the original 80 classes from the COCO dataset \cite{lin2015microsoftcococommonobjects}, we decided to re-evaluate its performance on the whole COCO dataset to quantify how much the finetuning process affected the original pre-trained model's performance. The results are shown Table \ref{tab:confusion-matrix-results}, where positive values are desirable for diagonal entries (indicating correct classifications), and negative values are preferred for off-diagonal entries (indicating reduced misclassifications).

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{images/final/yolo_confusion_matrix_subset.pdf}
%     \caption{Confusion matrix of the YOLOv9 model on a subset of 10 classes from the original COCO dataset \cite{lin2015microsoftcococommonobjects}, plus our \emph{fretboard} class.}
%     \label{fig:yolo-diff-confusion-matrix}
% \end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \toprule
        \textbf{Confusion Matrix Pos.} & \textbf{Positive} & \textbf{Negative} \\
        \midrule
        Diagonal                       & 1.41\%            & 5.38\%            \\
        \midrule
        Off-diagonal                   & 4.10\%            & 21.11\%           \\
        \bottomrule
    \end{tabular}
    \caption{Absolute sums of values (as \%) after taking the element-wise difference between the final confusion matrix obtained after finetuning the YOLOv9 model for our \emph{fretboard} class and the original confusion matrix of the pre-trained version on the COCO dataset. These values mean that, for the diagonal entries where the difference was positive, the model improved by 1.41\%, while for the off-diagonal entries where the difference was negative, the model improved by 21.11\%.}
    \label{tab:confusion-matrix-results}
\end{table}



\subsection{Guitar Chord Classification}
To evaluate our approache against those in the original paper, we implemented the InceptionResNetv2 model as described by the authors. After training the model using the hyperparameters provided by \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024} on our dataset, we obtained the results shown in Table \ref{tab:handpose-classifier-results}, which provided us with a baseline to compare our models against.

\begin{figure}[h]
    \centering
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/final/occlusion_untrained.pdf}
    \end{subfigure}
    \begin{subfigure}[t]{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/final/occlusion_trained.pdf}
    \end{subfigure}
    \caption{Occlusion-based attribution \cite{kokhlikyan2020captum} for model interpretability on a 74 $\times$ 389 input image using a stride of 8 and a sliding window of 30 $\times$ 30. \textbf{Top}: Untrained DINOv2 model. \textbf{Bottom}: Our DINOv2 model.}
    \label{fig:chord-classifier-visualization-fretboard}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/final/occlusion_trained_full.pdf}
    \caption{Our DINOv2 model on a 360x640 input image using a stride of 20 and a sliding window of 60x60.}
    \label{fig:chord-classifier-visualization-fretboard-2}
\end{figure}

\subsubsection{Hand Pose Estimation + Classifier}
Surprisingly, this approach performed well, achieving good accuracy during validation and testing on two datasets. However, the model struggled to generalize to the third dataset, which was created by us. This outcome was anticipated, as the samples in our dataset were out of the training distribution, and the model lacked the complexity needed to generalize to such data.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model}    & \textbf{GC}      & \textbf{GCT}     & \textbf{GCO}     \\
        \midrule
        InceptionResNetv2 & 83.56\%          & 68.63\%          & 15.57\%          \\
        \midrule
        SVM               & \textbf{95.27\%} & \textbf{85.71\%} & \textbf{18.61\%} \\
        Random Forest     & 93.35\%          & 52.41\%          & 16.16\%          \\
        MLP               & 89.44\%          & 78.57\%          & 14.39\%          \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy of the Hand Pose Estimation + Classifier in the test set of different datasets. The following parameters where used: \texttt{SVM (C = 300)}, \texttt{Random Forest (n\_estimators = 200)}, and \texttt{MLP (hidden\_layer\_sizes = (100, 256, 100))}. Datasets used: \textbf{GC}: Guitar\_Chords, \textbf{GCT}: Guitar\_Chords\_Tiny, \textbf{GCO}: Guitar\_Chords\_Ours.}
    \label{tab:handpose-classifier-results}
\end{table}

\subsubsection{Classifier only approach}
To address this limitation of the previous approach, we decided to explore more complex models, such as Vision Transformers and DINOv2. The results of our experiments are summarized in Table \ref{tab:transformer-models-results}.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Model}    & \textbf{GC}       & \textbf{GCT}     & \textbf{GCO}      \\
        \midrule
        InceptionResNetv2 & 83.56\%           & 68.63\%          & 15.57\%           \\
        \midrule
        ViT-B/16          & \textbf{98.96} \% & 85.29\%          & 96.24\%           \\
        ViT-B/32          & 93.07\%           & 81.37\%          & 95.83\%           \\
        ViT-L/16          & 95.84\%           & 81.37\%          & 12.29\%           \\
        ViT-L/32          & 77.03\%           & 43.14\%          & 13.43\%           \\
        DINOv2-S          & 96.24\%           & 88.24\%          & \textbf{98.18} \% \\
        DINOv2-L          & 96.44\%           & \textbf{91.18}\% & 97.92\%           \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy of \textbf{Classifier only} apporach in the test set of different datasets.}
    \label{tab:transformer-models-results}
\end{table}

ViT models show varying performance across different datasets. The base models perform exceptionally well, with high accuracy on all datasets. However, the larger models do not exhibit the same performance. We argue that this is happening because the available data is not sufficient to train the large version of the models effectively. Additionally, we can also observe that the patch 16 versions of the ViT models perform better than the patch 32 versions. This is likely due to the fact that the patch 16 versions have a higher resolution, which is important for accurately distinguishing between different hand positions.

Moreover, both DINOv2 variants demonstrated strong and consistent performance across all datasets. The DINOv2-L model, in particular, achieved the highest accuracy on the \textbf{Guitar\_Chords\_Ours} dataset, slightly outperforming the small variant. The superior performance of DINOv2 can be attributed to its self-supervised learning approach. Unlike models pre-trained on ImageNet, which does not contain a specific class for \emph{hands,} DINOv2's self-supervised learning allows enables it to learn more generic and transferable representations, leading to better generalization in our task. This enhanced generalization is further supported by attention visualizations of the model when applied to images from \textbf{Guitar\_Chords\_Ours} dataset, where the model correctly focuses on the hand performing the fretting, Figure \ref{fig:chord-classifier-visualization-fretboard} and \ref{fig:chord-classifier-visualization-fretboard-2}. 

Overall, our proposed models outperformed the InceptionResNetv2 model, achieving higher accuracy across all datasets. This demonstrates the potential of using more advanced models for chord classification tasks.

\subsection{Audio Generation}
Unfortunately, the generated audio from our proposed method did not meet our expectations. The quality of the generated sound was not good enough to be used in a real-world scenario. There was a noticeable synchronization issue between the audio, and the music being played in the video as important aspects like strumming patterns and durations were not taken into account.

\section{Conclusion}
Throughout our work, we have explored different models and techniques to improve the performance of guitar chord recognition. \dhimitrios{Add something related to YOLO here} We showed that using a pre-trained self-supervised model, such as DINOv2, can provide better generalization compared to models pre-trained on ImageNet thanks to its ability to learn more generic and transferable representations. In addition, using more complex classification models, can also make the usage of the fretboard detection model obsolete as shown in the occlusion-based attribution visualizations, Figure \ref{fig:chord-classifier-visualization-fretboard-2} where the model was able to learn to focus on the fretting hand with only being trained with cropped images. However, this needs further investigation and more data to be confirmed as the majority of the existing data is a cropped version of the fretboard.

\section{Discussion}
Unfortunately, we were unable to achieve satisfactory sound quality from our proposed pipeline. Our proposed approach was rather simplistic and did not take into account the complexity of the sound generation process. Moving forward, we propose implementing more advanced audio processing techniques such as the one used in \cite{su2020audeo} or \cite{li2023melodydiffusion} while also improving on the synchronization aspect. These improvements would enable the generation of a more true-to-life sound and therefore achieve our final goal of creating an end-to-end pipeline for recognizing and reconstructing the sound from a silent video of someone playing the guitar.

    %%%%%%%%% REFERENCES
    {\small
        \bibliographystyle{ieee_fullname}
        \bibliography{references}
    }

\end{document}