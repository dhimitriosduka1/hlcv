% CVPR 2023 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
\usepackage{cvpr}      % To produce the REVIEW version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}

% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2023}

\begin{document}

\newcommand{\dhimitrios}[1]{\textcolor{red}{Dhimitrios: #1}}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Team \#16: From Strings to Sequences --- Classifying and Generating Music from Acoustic Guitar Notes}


\author{
  Camilo Martínez\\
  7057573\\
  \and
  Dhimitrios Duka\\
  7059153\\
  \and
  Honglu Ma\\
  7055053\\
}
\maketitle

%%%%%%%%% BODY TEXT
\section{Abstract}
% Should be inside \emph{} https://openaccess.thecvf.com/content/CVPR2022W/WMF/papers/Guarnera_On_the_Exploitation_of_Deepfake_Model_Recognition_CVPRW_2022_paper.pdf
\emph{Lorem ipsum dolor sit amet, consectetur adipiscing elit}

\section{Introduction}
Automatic chord recognition (ACR) is an information retrieval task that consists of automatically recognizing the chords played in a music piece whether it be an audio or video file. The ability to accurately recognize and identify chords is crucial for a variety of downstream applications such as music analysis, music transcription, or even restoration of corrupted musical performances. 

\dhimitrios{Maybe make the transition to the next paragraph better.}

Building on the work of \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024}, our research seeks to enhance the accuracy of existing ACR systems by leveraging state-of-the-art deep learning models and techniques. Specifically, we explore the potential of using YOLO \cite{redmon2016you} models for fretboard recognition, Vision Transformers (ViT) \cite{dosovitskiy2020image} and DINOv2 \cite{oquab2023dinov2} for chord recognition, as well as investigate the feasibility of using hand pose estimation models like MediaPipe \cite{zhang2020mediapipe} to improve the classification accuracy. Additionally, we intend to extend this work by implementing a chord-to-audio generation model, enabling the generation of audio directly from recognized chord labels.

\section{Related Work}  
Over recent decades, many different approaches have been proposed to tackle the ACR task. The first ACR system was introduced in the late 1999s by \cite{takuya1999realtime}, where LISP music was utilized to perform chord recognition at the signal level. Since then, many signal-level-based approaches have been introduced ranging from chromagram-based method \cite{stark2009real}, to deep chromagram extractors \cite{korzeniowski2016feature}, to spectrogram-based feature extraction methods utilizing recurrent neural networks \cite{boulanger2013audio}. However, due to the inherent shortcomings of the aural approach in handling highly timbre sounds \cite{du2023conditional}, improvements plateaued. 

Consequently, researchers began exploring alternative approaches. An interesting one was the utilization of hand patterns to perform chord classification. This method was heavily based on the fact  that humans often find it easier to recognize chords based on visual cues rather than auditory ones. \cite{su2020audeo} was able to successfully classify the chords being played in a guitar and later on produce sounds out of that information. Furthermore, \cite{tran2019cnn} and \cite{ooaku2018guitar} tried to replicate the same idea but applied to classical guitars. Even though the works used different deep learning techniques, the former used transfer learning while the latter used a standard DCNN with a pre-trained backbone, the overall workflow was the same. First, a model would be responsible for detecting the freating hand within the image and then, crop the image and send it to a classifier to determine the chord being played.

Inspired by \cite{tran2019cnn, ooaku2018guitar}, \cite{Kristian_Zaman_Tenoyo_Jodhinata_2024} employed a Single Shot Detection (SSD) model \cite{sandler2018mobilenetv2}, pre-trained on the EgoHands \cite{Bambach_2015_ICCV} and ImageNet \cite{deng2009imagenet} dataset to achieve fretboard detection and chords classification and achieved SOTA performance.

\dhimitrios{Add something related to our work}

% Despite  these  developments,  theAutomatic Chord Recognition (ACR) system has wit-nessed a comparatively slower adoption of the visualapproach.  It was not until 2018 that computer visionand deep learning integration were applied to ACR,as evidenced in two significant papers [20] [21].Fig.1:System  Configuration  of  Previous  RelatedWork.Both proposed systems exhibit similar workflows,as illustrated in Figure 1.  Each of them utilizes handpattern information to identify guitar chords.  Theydetect the hand within the image and then send thedetected region to a convolutional network for chord classification.  Despite using the same LeNet (Incep-tion) methodology, they yield distinct results.The preliminary system described in [20] attemptsto  identify  the  cropped  finger  pattern  by  automati-cally generating a corresponding musical score usingVaticJS.  Three  distinct  scenarios,  assigned  to  classclusters of three, five, and six, were utilized to evalu-ate the system’s effectiveness.  It documented a recog-nition rate of 55\% for a set of six chords.  (A, B, C,D, E, F, G). Significantly, the system’s performanceimproved when the class size was decreased:  a three-chord  classifier  demonstrated  a  recognition  rate  of 90\%,  while  a  five-chord  classifier  achieved  a  recog-nition rate of 70\%.The second study presented in [21] employs a dis-tinct architecture for hand detection despite employ-ing a comparable methodology.  The hand region iscropped by the system using deep-learning object de-tection, which divides the process into hand detectionand chord classification.  The hand detection modelunderwent  training  using  the  EgoHands  dataset  onthe MobileNet v2 network.  For comparison purposes,the  classification  model  was  evaluated  on  numerousnetworks.   Using  pre-trained  LeNet  from  ImageNetweights,  the  project  achieved  a  perfect  accuracy  ofone  hundred  percent,  according  to  the  report;  nev-ertheless,  it  performed  inadequately  on  new  testingdata.  Both studies have identified LeNet as the mosteffective network for this classification task, indicat-ing  that  it  may  serve  as  a  beneficial  foundation  forour  research  or  even  be  enhanced  through  our  ad-justments.Timothius  Tirtawanet  al.[24]  proposed  a  tech-nique for semantic segmentation using a ConditionalGenerative Adversarial Network (CGAN) with U-Netarchitecture to apply batik patterns to clothing in im-ages.  This study highlights the potential of Convo-lutional  Generative  Adversarial  Networks  (CGANs)in the fashion industry.  It contributes to the broadergoal of implementing deep learning techniques in thevisual arts, which is pertinent to visual guitar chordrecognition.The study by Widjojo et al.[25] presents an inno-vative approach to expedite car insurance claims inIndonesia  through  an  integrated  deep  learning  sys-tem.  The system employs deep transfer learning tosegment automobile damage, identify damaged com-ponents,  and  categorize  the  extent  of  the  damage.The segmentation tasks were performed using MaskR-CNN, while the classification tasks involved a com-parison of EfficientNet and MobileNetV2. To enhancethe  F1  score,  they  implemented  a  straightforwardCNN concatenation.  Notably, the model that exhib-ited the highest performance was MobileNetV2 withCNN modifications, achieving an impressive F1 scoreof 91\%.  This approach is consistent with our visualmethod for identifying guitar chords,  in which deeptransfer learning is crucial to attaining high precisionand productivity.Building upon the foundation laid by previous re-search,  our  work  aims  to  address  the  limitations  ofexisting ACR methods by leveraging the full range offretboard information and investigating the efficacy oftransfer learning in training the classification model.By proposing a unique architecture and rigorously as-sessing the system’s performance in a realistic testingenvironment, we contribute to the advancement of vi-sual ACR systems and their potential for future devel-opments in the field of MIR. Our approach pushes theboundaries of chord recognition accuracy and opens up new possibilities for integrating ACR systems invarious applications, from music education to interac-tive music experiences.  Through this work, we hopeto inspire further research and innovation in the ex-citing intersection of computer vision, deep learning,and music information retrieval.

\section{Datasets}
We identified a significant gap in available datasets for the task of guitar chord recognition, which made us create our own. We recorded 90-second videos for each chord in three different environments, ensuring high quality by capturing them in 4K resolution at 60 frames per second. From these videos, we extracted frames and downsampled them to a resolution of 640 $\times$ 360 pixels. This process generated approximately 30,000 frames per chord.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.3\textwidth]{images/final/Guitar-chords-ours_tsne_plot.png}
  \caption{The t-SNE plot of our dataset containing 14 chords. Each point represents a KNN-sampled frame, with the color indicating the corresponding chord label.}
  \label{fig:ours-tsne-plot}
\end{figure}

To increase the diversity of the dataset, we used two different sampling methods: simple random sampling and KNN-based sampling. In the former method, we selected 1,000 frames at random while in the latter, we used the KNN algorithm to choose 1,000 frames that were the most distinct from one another.

Unfortunately, both sampling strategies resulted in an overly simplistic dataset that failed to capture the real-world complexity of chords, as shown by Figure \ref{fig:ours-tsne-plot}. This resulted in poor model generalization. However, rather than abandoning our dataset, \textbf{Guitrar\_chords\_ours}, we used it as a test set to evaluate the generalizability of our model. In the end, we decided to use existing datasets \cite{guitar-chord-tvon8_dataset,guitar-chord-bounding-box_dataset, guitar-chord-handshape_dataset, guitar-chords-daewp_dataset} for training the models, merging them to create a more complex dataset, \textbf{Guitar\_chords}, which resulted in significantly better results.

This change in our approach necessitated a change in the scope of our chord recognition task. As a consequence of using existing datasets, we were limited to only seven chords in total—A, B, C, D, E, F, and G—down from the 14 chords originally planned.

\section{Method}

\subsection{Guitar Chord Classification}

\subsubsection{Hand Pose Estimation + Classifier}
First, we wanted to try a simple yet interesting approach. For a given sample image, we utilized a hand pose estimation model to extract the hand shape from it, which was then used as the input to a classifier.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{images/final/hand_pose_estimation_classifier.png}
  \caption{t-SNE plot of our dataset. Each point represents a sampled frame, and the color indicates the chord label. }
  \label{fig:ours-tsne-plot}
\end{figure}

We used MediaPipe to extract the hand shape followed by different classifiers—SVM \cite{cortes1995support}, Random Forest \cite{ho1995random}, and a simple MLP—to classify the chords. 

\subsubsection{Classifier only}
Next, we wanted to explote the potential benefits of using more advanced architectures to perform chord classfication. We decided to experiment with Vision Transformers (ViT), specifically ViT-B/16, ViT-B/32, ViT-L/16, and ViT-L/32, to assess how different configurations of patch sizes and model sizes would impact performance. Additionally, we were also interesed in evaluationg the effectiveness of pre-treained self-supervised models in our task, so we also included DINOv2-S and DINOv2-L in our experiments, which allowed us to compare their performance against the ViT models and explore whether self-supervised learning offers advantages in this domain.

\section{Experimental Results and Analyses}
\label{sec:results}

\subsection{Guitar Chord Classification}
To evaluate our approaches against those in the original paper, we implemented the InceptionResNetv2 model as described by the authors. After training the model on our datasets, we obtained the results shown in Table \ref{tab:handpose-classifier-results}, which provided us with a baseline to compare our models against.

\subsubsection{Hand Pose Estimation + Classifier}
The results of this approach are summarized in Table \ref{tab:handpose-classifier-results}.

Surprisingly, this approach performed well, achieving good accuracy during validation and testing on two datasets. However, the model struggled to generalize to the third dataset, which we created. This outcome was anticipated, as the samples in our dataset were out of distribution, and the model lacked the complexity needed to generalize effectively to such data.

\subsubsection{Classifier only}



\dhimitrios{Should we point out that Hand Pose Estimation + Classifier performed better than InceptionResNetv2?}

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{GC} & \textbf{GCT} & \textbf{GCO} \\
    \midrule
 InceptionResNetv2 & 83.56\% & 68.63\% & 15.57\% \\
    \midrule
 SVM & 95.27\% & 85.71\% & 18.61\% \\
 Random Forest & 93.35\% & 52.41\% & 16.16\%  \\
 MLP & 89.44\% & 78.57\% & 14.39\% \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy of the Hand Pose Estimation + Classifier in the test set of different datasets. \textbf{GC}: Guitar\_Chords, \textbf{GCT}: Guitar\_Chords\_Tiny, \textbf{GCO}: Guitar\_Chords\_Ours.}
  \label{tab:handpose-classifier-results}
\end{table}

\subsubsection{Classifier only}
In the previous experiments, we found out that the Hand Pose Estimation + Classifier approach lacked the robustness necessary to generalize effectively to out-of-distribution data. To address this limitation, we decided to explore more complex models, such as Vision Transformers (ViT) and DINOv2. We evaluated various configurations of these models, including ViT-B/16, ViT-B/32, ViT-L/16, ViT-L/32, DINOv2-S, and DINOv2-L. The results of these experiments are summarized in Table \ref{tab:transformer-models-results}.

\begin{table}[h]
  \centering
  \begin{tabular}{lccc}
    \toprule
    \textbf{Model} & \textbf{GC} & \textbf{GCT} & \textbf{GCO} \\
    \midrule
    InceptionResNetv2 & 83.56\% & 68.63\% & 15.57\% \\
    \midrule
    ViT-B/16 & 98.96\% & 85.29\% & 96.24\% \\
    ViT-B/32 & 93.07\% & 81.37\% & 95.83\% \\
    ViT-L/16 & 95.84\% & 81.37\% & 12.29\% \\
    ViT-L/32 & 77.03\% & 43.14\% & 13.43\% \\
    DINOv2-S & 96.24\% & 88.24\% & 98.18\% \\
    DINOv2-L & 96.44\% & 91.18\% & 97.92\% \\
    \bottomrule
  \end{tabular}
  \caption{Accuracy of Classifier only in the test set of different datasets.}
  \label{tab:transformer-models-results}
\end{table}

As can be seen from Table \ref{tab:transformer-models-results}, ViT models show varying performance across different datasets. The base models, ViT-B/16 and ViT-B/32, perform exceptionally well, with high accuracy on all datasets. However, the larger models, ViT-L/16 and ViT-L/32, do not outperform the base models, suggesting that increased model size does not necessarily correlate with improved performance for this particular task. We expect that the larger models are overfitting the data, as they have more parameters to learn from and our data is limited.

We can also see that the patch 16 versions of the ViT models perform better than the patch 32 versions. This is likely due to the fact that the patch 16 versions have a higher resolution, to capture fine details and maintain higher spatial resolution, which is important for accurately distinguishing between chord shapes and hand positions

Both DINOv2 variants, small and large, demonstrated strong and consistent performance across all datasets. The DINOv2-large model, in particular, achieved the highest accuracy (0.979) on the Guitar\_Chords\_Ours dataset, slightly outperforming the small variant.

The superior performance of DINOv2 can be attributed to its self-supervised learning approach. Unlike models pre-trained on ImageNet, which does not contain specific classes related to 'hands,' DINOv2's self-supervised learning allows it to develop more general and transferable representations, leading to better generalization in our specific task. This enhanced generalization is further supported by attention visualizations of the model when applied to images from the 'ours' dataset, where the model correctly focuses on the hand performing the fretting.

\section{Conclusion}

\section{Discussion}
Currently, we are missing a background class in the dataset.


 %%%%%%%%% REFERENCES
 {\small
  \bibliographystyle{ieee_fullname}
  \bibliography{references}
 }

\end{document}