{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import json\n",
    "import wandb\n",
    "import torch\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from roboflow import Roboflow\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, DatasetDict, load_metric\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer, AutoImageProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChordsDataset(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.coco = json.load(f)\n",
    "        \n",
    "        self.images = self.coco['images']\n",
    "        self.annotations = self.coco['annotations']\n",
    "        self.categories = self.coco['categories']\n",
    "\n",
    "        self.images = {img['id']: img for img in self.images}\n",
    "\n",
    "        self.nr_of_classes = len(self.categories)\n",
    "\n",
    "        # Create a mapping from image_id to annotations\n",
    "        self.image_to_label = {}\n",
    "        for annotation in self.annotations:\n",
    "            img = self.images[annotation['image_id']]\n",
    "            self.image_to_label[img[\"id\"]] = {\n",
    "                \"file_name\": img[\"file_name\"],\n",
    "                \"category\": annotation[\"category_id\"]\n",
    "            }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        metadata = self.image_to_label[idx]\n",
    "        img_path = os.path.join(self.root_dir, metadata[\"file_name\"])\n",
    "        image = Image.open(img_path).convert('RGB')        \n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert labels to tensor\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"label\": torch.tensor(metadata[\"category\"]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_roboflow_data(config):\n",
    "    \"\"\"\n",
    "    Download dataset from RoboFlow.\n",
    "    \"\"\"\n",
    "    roboflow_config = config['data']['roboflow']\n",
    "    roboflow = Roboflow(api_key=roboflow_config[\"api_key\"])\n",
    "    project = roboflow.workspace(roboflow_config[\"workspace\"]).project(roboflow_config[\"project\"])\n",
    "    version = project.version(roboflow_config[\"version\"])\n",
    "    dataset = version.download(model_format=roboflow_config[\"version_download\"])\n",
    "\n",
    "    dest_path = config['data']['path'] + \"/\" + dataset.name\n",
    "\n",
    "    if not os.path.exists(dest_path):\n",
    "        shutil.move(src=dataset.location, dst=dest_path)\n",
    "\n",
    "    print(f\"Dataset downloaded and extracted to {config['data']['path']}\")\n",
    "    return dataset, dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transform(aug_config, processor):\n",
    "    transform_list = []\n",
    "    \n",
    "    # Add transforms based on configuration\n",
    "    # if 'random_resize_crop' in aug_config:\n",
    "    #     transform_list.append(transforms.RandomResizedCrop(**aug_config['random_resize_crop']))\n",
    "    # if 'random_horizontal_flip' in aug_config:\n",
    "    #     transform_list.append(transforms.RandomHorizontalFlip(aug_config['random_horizontal_flip']))\n",
    "    # if 'color_jitter' in aug_config:\n",
    "    #     transform_list.append(transforms.ColorJitter(**aug_config['color_jitter']))\n",
    "    # if 'random_rotation' in aug_config:\n",
    "    #     transform_list.append(transforms.RandomRotation(aug_config['random_rotation']))\n",
    "    \n",
    "    # Always include resizing, ToTensor, and normalization\n",
    "    transform_list.extend([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize(mean=processor.image_mean, std=processor.image_std),\n",
    "    ])\n",
    "    \n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(config, processor):\n",
    "    train_transform = create_transform(config['data']['train_augmentation'], processor)\n",
    "    val_transform = create_transform(config['data'].get('val_augmentation', {}), processor)\n",
    "    \n",
    "    return train_transform, val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir, transform):\n",
    "    return datasets.ImageFolder(data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_images_by_class(src_ds_path, ds_final_path):\n",
    "    # List of subdirectories to process\n",
    "    sub_dirs = ['train', 'valid', 'test']\n",
    "\n",
    "    os.makedirs(ds_final_path, exist_ok=True)\n",
    "\n",
    "    for sub_dir in sub_dirs:\n",
    "        current_dir = os.path.join(src_ds_path, sub_dir)\n",
    "        \n",
    "        # List all files in the current directory\n",
    "        files = [f for f in os.listdir(current_dir) if os.path.isfile(os.path.join(current_dir, f))]\n",
    "        \n",
    "        for f in files:\n",
    "            # Get the first letter of the file\n",
    "            first_letter = f[0].upper()\n",
    "            \n",
    "            if not first_letter.isalpha():\n",
    "                continue\n",
    "\n",
    "            # Create a new directory for this letter if it doesn't exist\n",
    "            letter_dir = os.path.join(ds_final_path, first_letter)\n",
    "            if not os.path.exists(letter_dir):\n",
    "                os.makedirs(letter_dir)\n",
    "            \n",
    "            # Move the file to the new directory\n",
    "            src_path = os.path.join(current_dir, f)\n",
    "            dst_path = os.path.join(letter_dir, f)\n",
    "            shutil.move(src_path, dst_path)\n",
    "    \n",
    "    shutil.rmtree(src_ds_path)\n",
    "\n",
    "    # Idk why the fuck this script created a copy of the Project dir\n",
    "    # shutil.rmtree(\"src/classification/Project\")\n",
    "    print(\"Image organization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_run_config = \"config.yml\"\n",
    "f_wandb_config = \"wandb.yml\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config(f_run_config)\n",
    "wandb_config = load_config(f_wandb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Guitar-Chord-1 to coco:: 100%|██████████| 166698/166698 [00:22<00:00, 7340.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Guitar-Chord-1 in coco:: 100%|██████████| 2525/2525 [00:01<00:00, 1389.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded and extracted to ./Project/src/classification/dataset\n",
      "Image organization complete!\n"
     ]
    }
   ],
   "source": [
    "# Download data from RoboFlow if specified\n",
    "if config['data'].get('use_roboflow', False):\n",
    "    _, location = download_roboflow_data(config)\n",
    "\n",
    "dataset_name = \"Guitar-Chords\"\n",
    "\n",
    "organize_images_by_class(location, \"datasets/\" + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdhimitrios-duka1\u001b[0m (\u001b[33mhwga-cj\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dhimitriosduka/Documents/UdS/SoSe 2024/High-Level Computer Vision/Assignments/hlcv/Project/src/classification/wandb/run-20240715_140550-0cn22v4v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hwga-cj/chord-classification/runs/0cn22v4v' target=\"_blank\">ViT-ft-ztizzm7f</a></strong> to <a href='https://wandb.ai/hwga-cj/chord-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hwga-cj/chord-classification' target=\"_blank\">https://wandb.ai/hwga-cj/chord-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hwga-cj/chord-classification/runs/0cn22v4v' target=\"_blank\">https://wandb.ai/hwga-cj/chord-classification/runs/0cn22v4v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hwga-cj/chord-classification/runs/0cn22v4v?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f51f413c1d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize wandb\n",
    "wandb.require(\"core\")\n",
    "wandb.init(\n",
    "    project=wandb_config[\"project\"],\n",
    "    name=wandb_config['name'] + \"-\" + wandb.util.generate_id(),\n",
    "    config=wandb_config,\n",
    "    entity=wandb_config[\"entity\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    " # Load pre-trained model and processor\n",
    "model = ViTForImageClassification.from_pretrained(config['model']['pretrained_weights'])\n",
    "processor = ViTImageProcessor.from_pretrained(config['model']['pretrained_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transforms\n",
    "train_transform, base_transform = get_transforms(config, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 2517/2517 [00:00<00:00, 85562.66files/s]\n",
      "Generating train split: 2517 examples [00:00, 4669.24 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 1761\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['image', 'label'],\n",
       "        num_rows: 378\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the ds\n",
    "ds = load_dataset(\"imagefolder\", data_dir=\"datasets/Guitar-Chords\")\n",
    "\n",
    "# Split the data\n",
    "ds = ds['train'].train_test_split(test_size=0.3, stratify_by_column=\"label\")  # 70% train, 30% test\n",
    "ds_test = ds['test'].train_test_split(test_size=0.5, stratify_by_column=\"label\")  # 30% test --> 15% valid, 15% test\n",
    "ds = DatasetDict({\n",
    "    'train': ds['train'],\n",
    "    'test': ds_test['test'],\n",
    "    'valid': ds_test['train']\n",
    "})\n",
    "    \n",
    "del ds_test\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['A', 'B', 'C', 'D', 'E', 'F', 'G'], id=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ds['train'].features['label']\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(batch):\n",
    "    # Resize the images to the desired size\n",
    "    resized_images = [transforms.Resize((224, 224))(x.convert(\"RGB\")) for x in batch['image']]\n",
    "\n",
    "    # Convert resized images to pixel values\n",
    "    inputs = processor(resized_images, return_tensors='pt')\n",
    "        \n",
    "    # Don't forget to include the labels!\n",
    "    inputs['label'] = batch['label']\n",
    "\n",
    "    return inputs\n",
    "\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(model_name_or_path)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels.names),\n",
    "    id2label={str(i): c for i, c in enumerate(labels.names)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels.names)},\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "  0%|          | 1/555 [00:35<5:27:00, 35.42s/it]"
     ]
    }
   ],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config['training']['output_dir'],\n",
    "    num_train_epochs=config['training']['num_epochs'],\n",
    "    per_device_train_batch_size=config['training']['batch_size'],\n",
    "    per_device_eval_batch_size=config['training']['batch_size'],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=float(config['training']['learning_rate']),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "# Define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"valid\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# # Save the fine-tuned model\n",
    "# trainer.save_model(config['training']['final_model_path'])\n",
    "\n",
    "# Close wandb run\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlcv-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
