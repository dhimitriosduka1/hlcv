{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just so that you don't have to restart the notebook with every change.\n",
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from collections import deque, Counter\n",
    "from common import utils\n",
    "from ultralytics import YOLO\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "# Useful constants\n",
    "CURRENT_DIR = os.getcwd()\n",
    "IMAGES_DIR = os.path.join(CURRENT_DIR, \"images\")\n",
    "VIDEOS_DIR = os.path.join(CURRENT_DIR, \"videos\")\n",
    "CHORD_CLASSIFIER_MODEL_DIR = os.path.join(CURRENT_DIR, \"chord-classifier-model\")\n",
    "FRETBOARD_RECOGNIZER_MODEL_DIR = os.path.join(CURRENT_DIR, \"fretboard-recognizer-model\")\n",
    "\n",
    "chord_clf_model_path = utils.find_files(CHORD_CLASSIFIER_MODEL_DIR, [\".safetensors\", \".pt\"])\n",
    "chord_clf_config_path = utils.find_files(CHORD_CLASSIFIER_MODEL_DIR, [\".json\"])\n",
    "fretboard_rec_model_path = utils.find_files(FRETBOARD_RECOGNIZER_MODEL_DIR, [\".safetensors\", \".pt\"])\n",
    "fretboard_rec_config_path = utils.find_files(FRETBOARD_RECOGNIZER_MODEL_DIR, [\".json\"])\n",
    "\n",
    "utils.ensure_files_exist(\n",
    "    chord_clf_model_path,\n",
    "    fretboard_rec_model_path,\n",
    "    chord_clf_config_path,\n",
    "    fretboard_rec_config_path,\n",
    "    names=[\n",
    "        \"Chord Classifier model\",\n",
    "        \"Fretboard Recognizer model\",\n",
    "        \"Chord Classifier config\",\n",
    "        \"Fretboard Recognizer config\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Chord Classifier model\n",
    "chord_clf_model = utils.load_model(chord_clf_model_path, config_path=chord_clf_config_path)\n",
    "\n",
    "# Load Fretboard Recognizer model\n",
    "fretboard_rec_model = utils.load_model(fretboard_rec_model_path, config_path=fretboard_rec_config_path, custom_class=YOLO)\n",
    "\n",
    "print(\"Models loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(\n",
    "        video_path,\n",
    "        chord_clf_model=None, \n",
    "        feature_extractor=None,\n",
    "        fretboard_rec_model=None\n",
    "):\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    print(f\"Video FPS: {fps}\")\n",
    "\n",
    "    recent_classifications = deque(maxlen=fps)\n",
    "    \n",
    "    current_frame = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        current_frame += 1\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert to PIL Image\n",
    "        pil_image = Image.fromarray(rgb_frame)\n",
    "\n",
    "        # Perform inference on the fretboard recognizer model\n",
    "        results = fretboard_rec_model.predict(pil_image)[0].boxes\n",
    "        indices = (results.cls == 80).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        if len(indices) > 0:\n",
    "            # Get the bounding box with the highest confidence\n",
    "            max_conf_index = results.conf[indices].argmax()\n",
    "            result = results.data[indices[max_conf_index]]\n",
    "\n",
    "            # Increase bounding box size by 90% :(\n",
    "            x1, y1, x2, y2 = result[:4]\n",
    "            width = x2 - x1\n",
    "            height = y2 - y1\n",
    "            increase_x = width * 0.90 / 2\n",
    "            increase_y = height * 0.90 / 2\n",
    "\n",
    "            new_x1 = max(0, x1 - increase_x)\n",
    "            new_y1 = max(0, y1 - increase_y)\n",
    "            new_x2 = min(pil_image.width, x2 + increase_x)\n",
    "            new_y2 = min(pil_image.height, y2 + increase_y)\n",
    "\n",
    "            # Crop the fretboard with increased bounding box\n",
    "            pil_image = pil_image.crop(np.array([new_x1, new_y1, new_x2, new_y2]))\n",
    "\n",
    "        # # Optional: Display the cropped image\n",
    "        # plt.imshow(pil_image)\n",
    "        # plt.show()\n",
    "\n",
    "        # Preprocess the image\n",
    "        inputs = feature_extractor(images=pil_image, return_tensors=\"pt\")\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            outputs = chord_clf_model(**inputs)\n",
    "\n",
    "        probabilities = F.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        # Get the predicted class\n",
    "        predicted_class_idx = probabilities.argmax(-1).item()\n",
    "        predicted_class = chord_clf_model.config.id2label[predicted_class_idx]\n",
    "\n",
    "        # Add the prediction to recent classifications\n",
    "        recent_classifications.append(predicted_class)\n",
    "\n",
    "        # If we have collected enough frames, determine the most common classification\n",
    "        if len(recent_classifications) == fps:\n",
    "            print(recent_classifications)\n",
    "            most_common_class = Counter(recent_classifications).most_common(1)[0][0]\n",
    "            print(f\"Frame {current_frame}: Most common classification in last {fps} frames: {most_common_class}\")\n",
    "            recent_classifications.clear()\n",
    "        \n",
    "        # Optional: Print progress\n",
    "        if current_frame % 100 == 0:\n",
    "            print(f\"Processed {current_frame}/{current_frame} frames\")\n",
    "    \n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = \"/home/dhimitriosduka/Videos/Screencasts/Screencast from 2024-08-26 11-52-18.mp4\"\n",
    "\n",
    "feature_extractor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\")\n",
    "\n",
    "process_video(\n",
    "    video_path,\n",
    "    chord_clf_model=chord_clf_model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    fretboard_rec_model=fretboard_rec_model \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hlcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
